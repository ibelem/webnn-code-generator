{
  "identifier": "mobileclip_s0_fp16.onnx",
  "format": "ONNX v7",
  "producer": "pytorch 1.13.1",
  "version": "0",
  "functions": [],
  "imports": [
    "ai.onnx v14"
  ],
  "metadata": [],
  "graph": [
    {
      "name": "torch_jit",
      "inputs": [
        {
          "name": "input_ids",
          "value": [
            {
              "name": "input_ids",
              "type": {
                "dataType": "int64",
                "shape": {
                  "dimensions": [
                    "batch_size",
                    "sequence_length"
                  ]
                }
              }
            }
          ]
        }
      ],
      "outputs": [
        {
          "name": "text_embeds",
          "value": [
            {
              "name": "text_embeds",
              "type": {
                "dataType": "float32",
                "shape": {
                  "dimensions": [
                    "batch_size",
                    512
                  ]
                }
              }
            }
          ]
        }
      ],
      "nodes": [
        {
          "name": "/embedding_layer/Gather",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "embedding_layer.weight",
                  "initializer": {
                    "name": "embedding_layer.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          49408,
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        49408,
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "input_ids",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        "batch_size",
                        "sequence_length"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/embedding_layer/Gather_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "batch_size",
                        "sequence_length",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/Shape",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/embedding_layer/Gather_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "batch_size",
                        "sequence_length",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "shape",
              "value": [
                {
                  "name": "/Shape_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Shape",
            "module": "ai.onnx",
            "version": 13,
            "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "shape",
                "type": "T1",
                "description": "Shape of the input tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Input tensor can be of arbitrary type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain output to int64 tensor.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "shape",
                "code": "x = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n).astype(np.float32)\ntest_shape(\"_example\", x)  # preserve names of original test cases\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\n\ntest_shape(\"\", x)  # preserve names of original test cases\n\ntest_shape(\"_start_1\", x, start=1)\n\ntest_shape(\"_end_1\", x, end=1)\n\ntest_shape(\"_start_negative_1\", x, start=-1)\n\ntest_shape(\"_end_negative_1\", x, end=-1)\n\ntest_shape(\"_start_1_end_negative_1\", x, start=1, end=-1)\n\ntest_shape(\"_start_1_end_2\", x, start=1, end=2)\n\ntest_shape(\"_clip_start\", x, start=-10)\n\ntest_shape(\"_clip_end\", x, end=10)"
              }
            ]
          }
        },
        {
          "name": "/Gather",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/Shape_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/positional_embedding/pos_embed/Unsqueeze",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_90",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/positional_embedding/pos_embed/Unsqueeze_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/positional_embedding/pos_embed/Concat",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "inputs",
              "value": [
                {
                  "name": "/positional_embedding/pos_embed/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 1,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/positional_embedding/pos_embed/Unsqueeze_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/positional_embedding/pos_embed/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 2,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "concat_result",
              "value": [
                {
                  "name": "/positional_embedding/pos_embed/Concat_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Concat",
            "module": "ai.onnx",
            "version": 13,
            "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": true,
                "description": "Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs).."
              }
            ],
            "inputs": [
              {
                "name": "inputs",
                "type": "T",
                "list": true,
                "description": "List of tensors for concatenation"
              }
            ],
            "min_input": 1,
            "max_input": 2147483647,
            "outputs": [
              {
                "name": "concat_result",
                "type": "T",
                "description": "Concatenated tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - &#8734;",
            "type_constraints": [
              {
                "description": "Constrain output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "concat",
                "code": "test_cases: dict[str, Sequence[Any]] = {\n    \"1d\": ([1, 2], [3, 4]),\n    \"2d\": ([[1, 2], [3, 4]], [[5, 6], [7, 8]]),\n    \"3d\": (\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        [[[9, 10], [11, 12]], [[13, 14], [15, 16]]],\n    ),\n}\n\nfor test_case, values_ in test_cases.items():\n    values = [np.asarray(v, dtype=np.float32) for v in values_]\n    for i in range(len(values[0].shape)):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_\" + str(i),\n        )\n\n    for i in range(-len(values[0].shape), 0):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_negative_\" + str(abs(i)),\n        )"
              }
            ],
            "category": "Tensor"
          }
        },
        {
          "name": "/positional_embedding/pos_embed/Reshape",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "positional_embedding.pos_embed.pos_embed",
                  "initializer": {
                    "name": "positional_embedding.pos_embed.pos_embed",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          1,
                          1,
                          77,
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        1,
                        1,
                        77,
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "shape",
              "value": [
                {
                  "name": "/positional_embedding/pos_embed/Concat_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reshaped",
              "value": [
                {
                  "name": "/positional_embedding/pos_embed/Reshape_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__0",
                        "unk__1",
                        "unk__2"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "allowzero",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Reshape",
            "module": "ai.onnx",
            "version": 14,
            "description": "Reshape the input tensor similar to numpy.reshape.\nFirst input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\nAt most one dimension of the new shape can be -1. In this case, the value is\ninferred from the size of the tensor and the remaining dimensions. A dimension\ncould also be 0, in which case the actual dimension value is unchanged (i.e. taken\nfrom the input tensor). If 'allowzero' is set, and the new shape includes 0, the\ndimension will be set explicitly to zero (i.e. not taken from input tensor).\nShape (second input) could be an empty shape, which means converting to a scalar.\nThe input tensor's shape and the output tensor's shape are required to have the same number of elements.\n\nIf the attribute 'allowzero' is set, it is invalid for the specified shape to\ncontain both a zero value and -1, as the value of the dimension corresponding\nto -1 cannot be determined uniquely.\n",
            "attributes": [
              {
                "name": "allowzero",
                "type": "int64",
                "required": false,
                "description": "(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              },
              {
                "name": "shape",
                "type": "tensor(int64)",
                "description": "Specified shape for output."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "reshaped",
                "type": "T",
                "description": "Reshaped data."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "allowzero",
                "code": "original_shape = [0, 3, 4]\ntest_cases = {\n    \"allowzero_reordered\": np.array([3, 4, 0], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n        allowzero=1,  # if allowzero=1, final shape = (3, 4, 0)\n        # if allowzero=0, final shape = (3, 4, 4)\n    )\n\n    reshaped = reshape_reference_implementation(data, shape, allowzero=1)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              },
              {
                "summary": "reshape",
                "code": "original_shape = [2, 3, 4]\ntest_cases = {\n    \"reordered_all_dims\": np.array([4, 2, 3], dtype=np.int64),\n    \"reordered_last_dims\": np.array([2, 4, 3], dtype=np.int64),\n    \"reduced_dims\": np.array([2, 12], dtype=np.int64),\n    \"extended_dims\": np.array([2, 3, 2, 2], dtype=np.int64),\n    \"one_dim\": np.array([24], dtype=np.int64),\n    \"negative_dim\": np.array([2, -1, 2], dtype=np.int64),\n    \"negative_extended_dims\": np.array([-1, 2, 3, 4], dtype=np.int64),\n    \"zero_dim\": np.array([2, 0, 4, 1], dtype=np.int64),\n    \"zero_and_negative_dim\": np.array([2, 0, 1, -1], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n    )\n\n    reshaped = reshape_reference_implementation(data, shape)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              }
            ],
            "category": "Shape"
          }
        },
        {
          "name": "/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/positional_embedding/pos_embed/Reshape_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__0",
                        "unk__1",
                        "unk__2"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__0",
                        "unk__1",
                        "unk__2"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/embedding_layer/Gather_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "batch_size",
                        "sequence_length",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__0",
                        "unk__1",
                        "unk__2"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__4",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.0/Transpose",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__4",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.0/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        "unk__4"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "2",
                  "1"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.0/Unsqueeze",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.0/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        "unk__4"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "/transformer.0/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 2,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.0/Unsqueeze_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__4"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.0/token_mixer/reparam_conv/Conv",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.0/Unsqueeze_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__4"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "W",
              "value": [
                {
                  "name": "transformer.0.token_mixer.reparam_conv.weight",
                  "initializer": {
                    "name": "transformer.0.token_mixer.reparam_conv.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          1,
                          1,
                          11
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        1,
                        1,
                        11
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.0.token_mixer.reparam_conv.bias",
                  "initializer": {
                    "name": "transformer.0.token_mixer.reparam_conv.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.0/token_mixer/reparam_conv/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__5"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "dilations",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            },
            {
              "name": "group",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "512"
              }
            },
            {
              "name": "kernel_shape",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "11"
                ]
              }
            },
            {
              "name": "pads",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "5",
                  "0",
                  "5"
                ]
              }
            },
            {
              "name": "strides",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            }
          ],
          "type": {
            "name": "Conv",
            "module": "ai.onnx",
            "version": 11,
            "description": "The convolution operator consumes an input tensor and a filter, and\ncomputes the output.",
            "attributes": [
              {
                "name": "auto_pad",
                "type": "string",
                "required": false,
                "default": "NOTSET",
                "description": "auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER."
              },
              {
                "name": "dilations",
                "type": "int64[]",
                "required": false,
                "description": "dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis."
              },
              {
                "name": "group",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "number of groups input channels and output channels are divided into."
              },
              {
                "name": "kernel_shape",
                "type": "int64[]",
                "required": false,
                "description": "The shape of the convolution kernel. If not present, should be inferred from input W."
              },
              {
                "name": "pads",
                "type": "int64[]",
                "required": false,
                "description": "Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis."
              },
              {
                "name": "strides",
                "type": "int64[]",
                "required": false,
                "description": "Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis."
              }
            ],
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input data tensor from previous layer; has size (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and width. Note that this is for the 2D image. Otherwise the size is (N x C x D1 x D2 ... x Dn). Optionally, if dimension denotation is in effect, the operation expects input data tensor to arrive with the dimension denotation of [DATA_BATCH, DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE ...]."
              },
              {
                "name": "W",
                "type": "T",
                "description": "The weight tensor that will be used in the convolutions; has size (M x C/group x kH x kW), where C is the number of channels, and kH and kW are the height and width of the kernel, and M is the number of feature maps. For more than 2 dimensions, the kernel shape will be (M x C/group x k1 x k2 x ... x kn), where (k1 x k2 x ... kn) is the dimension of the kernel. Optionally, if dimension denotation is in effect, the operation expects the weight tensor to arrive with the dimension denotation of [FILTER_OUT_CHANNEL, FILTER_IN_CHANNEL, FILTER_SPATIAL, FILTER_SPATIAL ...]. Assuming zero based indices for the shape array, X.shape[1] == (W.shape[1] * group) == C and W.shape[0] mod G == 0. Or in other words FILTER_IN_CHANNEL multiplied by the number of groups should be equal to DATA_CHANNEL and the number of feature maps M should be a multiple of the number of groups G."
              },
              {
                "name": "B",
                "type": "T",
                "option": "optional",
                "description": "Optional 1D bias to be added to the convolution, has size of M."
              }
            ],
            "min_input": 2,
            "max_input": 3,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output data tensor that contains the result of the convolution. The output dimensions are functions of the kernel size, stride size, and pad lengths."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "2 - 3",
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "conv",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[1, 1, 1, 1],\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 21.0, 27.0, 33.0, 24.0],  # (1, 1, 5, 5) output tensor\n                [33.0, 54.0, 63.0, 72.0, 51.0],\n                [63.0, 99.0, 108.0, 117.0, 81.0],\n                [93.0, 144.0, 153.0, 162.0, 111.0],\n                [72.0, 111.0, 117.0, 123.0, 84.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_basic_conv_with_padding\",\n)\n\n# Convolution without padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[0, 0, 0, 0],\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 63.0, 72.0],  # (1, 1, 3, 3) output tensor\n                [99.0, 108.0, 117.0],\n                [144.0, 153.0, 162.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_basic_conv_without_padding\",\n)"
              },
              {
                "summary": "conv_with_autopad_same",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with auto_pad='SAME_LOWER' and strides=2\nnode = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    auto_pad=\"SAME_LOWER\",\n    kernel_shape=[3, 3],\n    strides=[2, 2],\n)\ny = np.array(\n    [[[[12.0, 27.0, 24.0], [63.0, 108.0, 81.0], [72.0, 117.0, 84.0]]]]\n).astype(np.float32)\nexpect(node, inputs=[x, W], outputs=[y], name=\"test_conv_with_autopad_same\")"
              },
              {
                "summary": "conv_with_strides",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 7, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n                [25.0, 26.0, 27.0, 28.0, 29.0],\n                [30.0, 31.0, 32.0, 33.0, 34.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with strides=2 and padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 1, 1, 1],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 27.0, 24.0],  # (1, 1, 4, 3) output tensor\n                [63.0, 108.0, 81.0],\n                [123.0, 198.0, 141.0],\n                [112.0, 177.0, 124.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_conv_with_strides_padding\",\n)\n\n# Convolution with strides=2 and no padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[0, 0, 0, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 72.0],  # (1, 1, 3, 2) output tensor\n                [144.0, 162.0],\n                [234.0, 252.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_conv_with_strides_no_padding\",\n)\n\n# Convolution with strides=2 and padding only along one dimension (the H dimension in NxCxHxW tensor)\nnode_with_asymmetric_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 0, 1, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_asymmetric_padding = np.array(\n    [\n        [\n            [\n                [21.0, 33.0],  # (1, 1, 4, 2) output tensor\n                [99.0, 117.0],\n                [189.0, 207.0],\n                [171.0, 183.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_asymmetric_padding,\n    inputs=[x, W],\n    outputs=[y_with_asymmetric_padding],\n    name=\"test_conv_with_strides_and_asymmetric_padding\",\n)"
              }
            ],
            "category": "Layer"
          }
        },
        {
          "name": "/transformer.0/convffn/conv/conv/Conv",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.0/token_mixer/reparam_conv/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__5"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "W",
              "value": [
                {
                  "name": "onnx::Conv_531",
                  "initializer": {
                    "name": "onnx::Conv_531",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          1,
                          1,
                          11
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        1,
                        1,
                        11
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::Conv_532",
                  "initializer": {
                    "name": "onnx::Conv_532",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.0/convffn/conv/conv/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__6"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "dilations",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            },
            {
              "name": "group",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "512"
              }
            },
            {
              "name": "kernel_shape",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "11"
                ]
              }
            },
            {
              "name": "pads",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "5",
                  "0",
                  "5"
                ]
              }
            },
            {
              "name": "strides",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            }
          ],
          "type": {
            "name": "Conv",
            "module": "ai.onnx",
            "version": 11,
            "description": "The convolution operator consumes an input tensor and a filter, and\ncomputes the output.",
            "attributes": [
              {
                "name": "auto_pad",
                "type": "string",
                "required": false,
                "default": "NOTSET",
                "description": "auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER."
              },
              {
                "name": "dilations",
                "type": "int64[]",
                "required": false,
                "description": "dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis."
              },
              {
                "name": "group",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "number of groups input channels and output channels are divided into."
              },
              {
                "name": "kernel_shape",
                "type": "int64[]",
                "required": false,
                "description": "The shape of the convolution kernel. If not present, should be inferred from input W."
              },
              {
                "name": "pads",
                "type": "int64[]",
                "required": false,
                "description": "Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis."
              },
              {
                "name": "strides",
                "type": "int64[]",
                "required": false,
                "description": "Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis."
              }
            ],
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input data tensor from previous layer; has size (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and width. Note that this is for the 2D image. Otherwise the size is (N x C x D1 x D2 ... x Dn). Optionally, if dimension denotation is in effect, the operation expects input data tensor to arrive with the dimension denotation of [DATA_BATCH, DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE ...]."
              },
              {
                "name": "W",
                "type": "T",
                "description": "The weight tensor that will be used in the convolutions; has size (M x C/group x kH x kW), where C is the number of channels, and kH and kW are the height and width of the kernel, and M is the number of feature maps. For more than 2 dimensions, the kernel shape will be (M x C/group x k1 x k2 x ... x kn), where (k1 x k2 x ... kn) is the dimension of the kernel. Optionally, if dimension denotation is in effect, the operation expects the weight tensor to arrive with the dimension denotation of [FILTER_OUT_CHANNEL, FILTER_IN_CHANNEL, FILTER_SPATIAL, FILTER_SPATIAL ...]. Assuming zero based indices for the shape array, X.shape[1] == (W.shape[1] * group) == C and W.shape[0] mod G == 0. Or in other words FILTER_IN_CHANNEL multiplied by the number of groups should be equal to DATA_CHANNEL and the number of feature maps M should be a multiple of the number of groups G."
              },
              {
                "name": "B",
                "type": "T",
                "option": "optional",
                "description": "Optional 1D bias to be added to the convolution, has size of M."
              }
            ],
            "min_input": 2,
            "max_input": 3,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output data tensor that contains the result of the convolution. The output dimensions are functions of the kernel size, stride size, and pad lengths."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "2 - 3",
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "conv",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[1, 1, 1, 1],\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 21.0, 27.0, 33.0, 24.0],  # (1, 1, 5, 5) output tensor\n                [33.0, 54.0, 63.0, 72.0, 51.0],\n                [63.0, 99.0, 108.0, 117.0, 81.0],\n                [93.0, 144.0, 153.0, 162.0, 111.0],\n                [72.0, 111.0, 117.0, 123.0, 84.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_basic_conv_with_padding\",\n)\n\n# Convolution without padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[0, 0, 0, 0],\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 63.0, 72.0],  # (1, 1, 3, 3) output tensor\n                [99.0, 108.0, 117.0],\n                [144.0, 153.0, 162.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_basic_conv_without_padding\",\n)"
              },
              {
                "summary": "conv_with_autopad_same",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with auto_pad='SAME_LOWER' and strides=2\nnode = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    auto_pad=\"SAME_LOWER\",\n    kernel_shape=[3, 3],\n    strides=[2, 2],\n)\ny = np.array(\n    [[[[12.0, 27.0, 24.0], [63.0, 108.0, 81.0], [72.0, 117.0, 84.0]]]]\n).astype(np.float32)\nexpect(node, inputs=[x, W], outputs=[y], name=\"test_conv_with_autopad_same\")"
              },
              {
                "summary": "conv_with_strides",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 7, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n                [25.0, 26.0, 27.0, 28.0, 29.0],\n                [30.0, 31.0, 32.0, 33.0, 34.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with strides=2 and padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 1, 1, 1],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 27.0, 24.0],  # (1, 1, 4, 3) output tensor\n                [63.0, 108.0, 81.0],\n                [123.0, 198.0, 141.0],\n                [112.0, 177.0, 124.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_conv_with_strides_padding\",\n)\n\n# Convolution with strides=2 and no padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[0, 0, 0, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 72.0],  # (1, 1, 3, 2) output tensor\n                [144.0, 162.0],\n                [234.0, 252.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_conv_with_strides_no_padding\",\n)\n\n# Convolution with strides=2 and padding only along one dimension (the H dimension in NxCxHxW tensor)\nnode_with_asymmetric_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 0, 1, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_asymmetric_padding = np.array(\n    [\n        [\n            [\n                [21.0, 33.0],  # (1, 1, 4, 2) output tensor\n                [99.0, 117.0],\n                [189.0, 207.0],\n                [171.0, 183.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_asymmetric_padding,\n    inputs=[x, W],\n    outputs=[y_with_asymmetric_padding],\n    name=\"test_conv_with_strides_and_asymmetric_padding\",\n)"
              }
            ],
            "category": "Layer"
          }
        },
        {
          "name": "/transformer.0/convffn/fc1/Conv",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.0/convffn/conv/conv/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__6"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "W",
              "value": [
                {
                  "name": "transformer.0.convffn.fc1.weight",
                  "initializer": {
                    "name": "transformer.0.convffn.fc1.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          2048,
                          512,
                          1,
                          1
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        2048,
                        512,
                        1,
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.0.convffn.fc1.bias",
                  "initializer": {
                    "name": "transformer.0.convffn.fc1.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          2048
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.0/convffn/fc1/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "dilations",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            },
            {
              "name": "group",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "1"
              }
            },
            {
              "name": "kernel_shape",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            },
            {
              "name": "pads",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "0",
                  "0",
                  "0"
                ]
              }
            },
            {
              "name": "strides",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            }
          ],
          "type": {
            "name": "Conv",
            "module": "ai.onnx",
            "version": 11,
            "description": "The convolution operator consumes an input tensor and a filter, and\ncomputes the output.",
            "attributes": [
              {
                "name": "auto_pad",
                "type": "string",
                "required": false,
                "default": "NOTSET",
                "description": "auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER."
              },
              {
                "name": "dilations",
                "type": "int64[]",
                "required": false,
                "description": "dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis."
              },
              {
                "name": "group",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "number of groups input channels and output channels are divided into."
              },
              {
                "name": "kernel_shape",
                "type": "int64[]",
                "required": false,
                "description": "The shape of the convolution kernel. If not present, should be inferred from input W."
              },
              {
                "name": "pads",
                "type": "int64[]",
                "required": false,
                "description": "Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis."
              },
              {
                "name": "strides",
                "type": "int64[]",
                "required": false,
                "description": "Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis."
              }
            ],
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input data tensor from previous layer; has size (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and width. Note that this is for the 2D image. Otherwise the size is (N x C x D1 x D2 ... x Dn). Optionally, if dimension denotation is in effect, the operation expects input data tensor to arrive with the dimension denotation of [DATA_BATCH, DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE ...]."
              },
              {
                "name": "W",
                "type": "T",
                "description": "The weight tensor that will be used in the convolutions; has size (M x C/group x kH x kW), where C is the number of channels, and kH and kW are the height and width of the kernel, and M is the number of feature maps. For more than 2 dimensions, the kernel shape will be (M x C/group x k1 x k2 x ... x kn), where (k1 x k2 x ... kn) is the dimension of the kernel. Optionally, if dimension denotation is in effect, the operation expects the weight tensor to arrive with the dimension denotation of [FILTER_OUT_CHANNEL, FILTER_IN_CHANNEL, FILTER_SPATIAL, FILTER_SPATIAL ...]. Assuming zero based indices for the shape array, X.shape[1] == (W.shape[1] * group) == C and W.shape[0] mod G == 0. Or in other words FILTER_IN_CHANNEL multiplied by the number of groups should be equal to DATA_CHANNEL and the number of feature maps M should be a multiple of the number of groups G."
              },
              {
                "name": "B",
                "type": "T",
                "option": "optional",
                "description": "Optional 1D bias to be added to the convolution, has size of M."
              }
            ],
            "min_input": 2,
            "max_input": 3,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output data tensor that contains the result of the convolution. The output dimensions are functions of the kernel size, stride size, and pad lengths."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "2 - 3",
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "conv",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[1, 1, 1, 1],\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 21.0, 27.0, 33.0, 24.0],  # (1, 1, 5, 5) output tensor\n                [33.0, 54.0, 63.0, 72.0, 51.0],\n                [63.0, 99.0, 108.0, 117.0, 81.0],\n                [93.0, 144.0, 153.0, 162.0, 111.0],\n                [72.0, 111.0, 117.0, 123.0, 84.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_basic_conv_with_padding\",\n)\n\n# Convolution without padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[0, 0, 0, 0],\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 63.0, 72.0],  # (1, 1, 3, 3) output tensor\n                [99.0, 108.0, 117.0],\n                [144.0, 153.0, 162.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_basic_conv_without_padding\",\n)"
              },
              {
                "summary": "conv_with_autopad_same",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with auto_pad='SAME_LOWER' and strides=2\nnode = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    auto_pad=\"SAME_LOWER\",\n    kernel_shape=[3, 3],\n    strides=[2, 2],\n)\ny = np.array(\n    [[[[12.0, 27.0, 24.0], [63.0, 108.0, 81.0], [72.0, 117.0, 84.0]]]]\n).astype(np.float32)\nexpect(node, inputs=[x, W], outputs=[y], name=\"test_conv_with_autopad_same\")"
              },
              {
                "summary": "conv_with_strides",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 7, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n                [25.0, 26.0, 27.0, 28.0, 29.0],\n                [30.0, 31.0, 32.0, 33.0, 34.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with strides=2 and padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 1, 1, 1],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 27.0, 24.0],  # (1, 1, 4, 3) output tensor\n                [63.0, 108.0, 81.0],\n                [123.0, 198.0, 141.0],\n                [112.0, 177.0, 124.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_conv_with_strides_padding\",\n)\n\n# Convolution with strides=2 and no padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[0, 0, 0, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 72.0],  # (1, 1, 3, 2) output tensor\n                [144.0, 162.0],\n                [234.0, 252.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_conv_with_strides_no_padding\",\n)\n\n# Convolution with strides=2 and padding only along one dimension (the H dimension in NxCxHxW tensor)\nnode_with_asymmetric_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 0, 1, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_asymmetric_padding = np.array(\n    [\n        [\n            [\n                [21.0, 33.0],  # (1, 1, 4, 2) output tensor\n                [99.0, 117.0],\n                [189.0, 207.0],\n                [171.0, 183.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_asymmetric_padding,\n    inputs=[x, W],\n    outputs=[y_with_asymmetric_padding],\n    name=\"test_conv_with_strides_and_asymmetric_padding\",\n)"
              }
            ],
            "category": "Layer"
          }
        },
        {
          "name": "/transformer.0/convffn/act/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.0/convffn/fc1/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.0/convffn/act/Erf",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Erf_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Erf",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the error function of the given input tensor element-wise.\n",
            "inputs": [
              {
                "name": "input",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "The error function of the input tensor computed element-wise. It has the same shape and type of the input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "erf",
                "code": "node = onnx.helper.make_node(\n    \"Erf\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.random.randn(1, 3, 32, 32).astype(np.float32)\ny = np.vectorize(math.erf)(x).astype(np.float32)\nexpect(node, inputs=[x], outputs=[y], name=\"test_erf\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.0/convffn/act/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Erf_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.0/convffn/act/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.0/convffn/fc1/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.0/convffn/act/Mul_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Constant_2_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Mul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.0/convffn/fc2/Conv",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.0/convffn/act/Mul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        2048,
                        1,
                        "unk__7"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "W",
              "value": [
                {
                  "name": "transformer.0.convffn.fc2.weight",
                  "initializer": {
                    "name": "transformer.0.convffn.fc2.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          2048,
                          1,
                          1
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        2048,
                        1,
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.0.convffn.fc2.bias",
                  "initializer": {
                    "name": "transformer.0.convffn.fc2.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.0/convffn/fc2/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__8"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "dilations",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            },
            {
              "name": "group",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "1"
              }
            },
            {
              "name": "kernel_shape",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            },
            {
              "name": "pads",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "0",
                  "0",
                  "0"
                ]
              }
            },
            {
              "name": "strides",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            }
          ],
          "type": {
            "name": "Conv",
            "module": "ai.onnx",
            "version": 11,
            "description": "The convolution operator consumes an input tensor and a filter, and\ncomputes the output.",
            "attributes": [
              {
                "name": "auto_pad",
                "type": "string",
                "required": false,
                "default": "NOTSET",
                "description": "auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER."
              },
              {
                "name": "dilations",
                "type": "int64[]",
                "required": false,
                "description": "dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis."
              },
              {
                "name": "group",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "number of groups input channels and output channels are divided into."
              },
              {
                "name": "kernel_shape",
                "type": "int64[]",
                "required": false,
                "description": "The shape of the convolution kernel. If not present, should be inferred from input W."
              },
              {
                "name": "pads",
                "type": "int64[]",
                "required": false,
                "description": "Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis."
              },
              {
                "name": "strides",
                "type": "int64[]",
                "required": false,
                "description": "Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis."
              }
            ],
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input data tensor from previous layer; has size (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and width. Note that this is for the 2D image. Otherwise the size is (N x C x D1 x D2 ... x Dn). Optionally, if dimension denotation is in effect, the operation expects input data tensor to arrive with the dimension denotation of [DATA_BATCH, DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE ...]."
              },
              {
                "name": "W",
                "type": "T",
                "description": "The weight tensor that will be used in the convolutions; has size (M x C/group x kH x kW), where C is the number of channels, and kH and kW are the height and width of the kernel, and M is the number of feature maps. For more than 2 dimensions, the kernel shape will be (M x C/group x k1 x k2 x ... x kn), where (k1 x k2 x ... kn) is the dimension of the kernel. Optionally, if dimension denotation is in effect, the operation expects the weight tensor to arrive with the dimension denotation of [FILTER_OUT_CHANNEL, FILTER_IN_CHANNEL, FILTER_SPATIAL, FILTER_SPATIAL ...]. Assuming zero based indices for the shape array, X.shape[1] == (W.shape[1] * group) == C and W.shape[0] mod G == 0. Or in other words FILTER_IN_CHANNEL multiplied by the number of groups should be equal to DATA_CHANNEL and the number of feature maps M should be a multiple of the number of groups G."
              },
              {
                "name": "B",
                "type": "T",
                "option": "optional",
                "description": "Optional 1D bias to be added to the convolution, has size of M."
              }
            ],
            "min_input": 2,
            "max_input": 3,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output data tensor that contains the result of the convolution. The output dimensions are functions of the kernel size, stride size, and pad lengths."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "2 - 3",
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "conv",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[1, 1, 1, 1],\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 21.0, 27.0, 33.0, 24.0],  # (1, 1, 5, 5) output tensor\n                [33.0, 54.0, 63.0, 72.0, 51.0],\n                [63.0, 99.0, 108.0, 117.0, 81.0],\n                [93.0, 144.0, 153.0, 162.0, 111.0],\n                [72.0, 111.0, 117.0, 123.0, 84.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_basic_conv_with_padding\",\n)\n\n# Convolution without padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[0, 0, 0, 0],\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 63.0, 72.0],  # (1, 1, 3, 3) output tensor\n                [99.0, 108.0, 117.0],\n                [144.0, 153.0, 162.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_basic_conv_without_padding\",\n)"
              },
              {
                "summary": "conv_with_autopad_same",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with auto_pad='SAME_LOWER' and strides=2\nnode = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    auto_pad=\"SAME_LOWER\",\n    kernel_shape=[3, 3],\n    strides=[2, 2],\n)\ny = np.array(\n    [[[[12.0, 27.0, 24.0], [63.0, 108.0, 81.0], [72.0, 117.0, 84.0]]]]\n).astype(np.float32)\nexpect(node, inputs=[x, W], outputs=[y], name=\"test_conv_with_autopad_same\")"
              },
              {
                "summary": "conv_with_strides",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 7, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n                [25.0, 26.0, 27.0, 28.0, 29.0],\n                [30.0, 31.0, 32.0, 33.0, 34.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with strides=2 and padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 1, 1, 1],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 27.0, 24.0],  # (1, 1, 4, 3) output tensor\n                [63.0, 108.0, 81.0],\n                [123.0, 198.0, 141.0],\n                [112.0, 177.0, 124.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_conv_with_strides_padding\",\n)\n\n# Convolution with strides=2 and no padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[0, 0, 0, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 72.0],  # (1, 1, 3, 2) output tensor\n                [144.0, 162.0],\n                [234.0, 252.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_conv_with_strides_no_padding\",\n)\n\n# Convolution with strides=2 and padding only along one dimension (the H dimension in NxCxHxW tensor)\nnode_with_asymmetric_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 0, 1, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_asymmetric_padding = np.array(\n    [\n        [\n            [\n                [21.0, 33.0],  # (1, 1, 4, 2) output tensor\n                [99.0, 117.0],\n                [189.0, 207.0],\n                [171.0, 183.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_asymmetric_padding,\n    inputs=[x, W],\n    outputs=[y_with_asymmetric_padding],\n    name=\"test_conv_with_strides_and_asymmetric_padding\",\n)"
              }
            ],
            "category": "Layer"
          }
        },
        {
          "name": "/transformer.0/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.0.layer_scale",
                  "initializer": {
                    "name": "transformer.0.layer_scale",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          1,
                          1
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        1,
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.0/convffn/fc2/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__8"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__8"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.0/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.0/token_mixer/reparam_conv/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__5"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__8"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__9"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.0/Squeeze",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        1,
                        "unk__9"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "/transformer.0/Constant_2_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 2,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "squeezed",
              "value": [
                {
                  "name": "/transformer.0/Squeeze_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        "unk__9"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Squeeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Remove single-dimensional entries from the shape of a tensor.\nTakes an input `axes` with a list of axes to squeeze.\nIf `axes` is not provided, all the single dimensions will be removed from\nthe shape. If an axis is selected with shape entry not equal to one, an error is raised.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensors with at least max(dims) dimensions."
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "option": "optional",
                "description": "List of integers indicating the dimensions to squeeze. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "min_input": 1,
            "max_input": 2,
            "outputs": [
              {
                "name": "squeezed",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - 2",
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "squeeze",
                "code": "node = onnx.helper.make_node(\n    \"Squeeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 4, 5).astype(np.float32)\naxes = np.array([0], dtype=np.int64)\ny = np.squeeze(x, axis=0)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_squeeze\")"
              },
              {
                "summary": "squeeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Squeeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2], dtype=np.int64)\ny = np.squeeze(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_squeeze_negative_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.0/Transpose_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.0/Squeeze_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        512,
                        "unk__9"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.0/Transpose_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "2",
                  "1"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.0/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.0/Transpose_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.0/ReduceMean",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.0/Sub",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sub",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary subtraction (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sub",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([3, 2, 1]).astype(np.float32)\nz = x - y  # expected output [-2., 0., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint32\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint64)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint64\")"
              },
              {
                "summary": "sub_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.0/Pow",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Z",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Pow",
            "module": "ai.onnx",
            "version": 13,
            "description": "Pow takes input data (Tensor<T>) and exponent Tensor, and\nproduces one output data (Tensor<T>) where the function `f(x) = x^exponent`,\nis applied to the data tensor elementwise.\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "First operand, base of the exponent."
              },
              {
                "name": "Y",
                "type": "T1",
                "description": "Second operand, power of the exponent."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Z",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input X and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain input Y types to float/int tensors.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "pow",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_example\")\n\nx = np.arange(60).reshape(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow\")"
              },
              {
                "summary": "pow_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array(2).astype(np.float32)\nz = pow(x, y)  # expected output [1., 4., 9.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_scalar\")\n\nnode = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\nx = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\ny = np.array([1, 2, 3]).astype(np.float32)\n# expected output [[1, 4, 27], [4, 25, 216]]\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_array\")"
              },
              {
                "summary": "types",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int32\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint64\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint32\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_int32\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.0/ReduceMean_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.0/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.0/Sqrt",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sqrt",
            "module": "ai.onnx",
            "version": 13,
            "description": "Square root takes one input data (Tensor<T>) and produces one output data\n(Tensor<T>) where the square root is, y = x^0.5, is applied to\nthe tensor elementwise. If x is negative, then it will return NaN.\n",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sqrt",
                "code": "node = onnx.helper.make_node(\n    \"Sqrt\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.array([1, 4, 9]).astype(np.float32)\ny = np.sqrt(x)  # expected output [1., 2., 3.]\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt_example\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\ny = np.sqrt(x)\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.0/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.0/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.1.pre_norm_mha.0.weight",
                  "initializer": {
                    "name": "transformer.1.pre_norm_mha.0.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.0/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.1.pre_norm_mha.0.bias",
                  "initializer": {
                    "name": "transformer.1.pre_norm_mha.0.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.0/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Shape",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Shape_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Shape",
            "module": "ai.onnx",
            "version": 13,
            "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "shape",
                "type": "T1",
                "description": "Shape of the input tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Input tensor can be of arbitrary type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain output to int64 tensor.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "shape",
                "code": "x = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n).astype(np.float32)\ntest_shape(\"_example\", x)  # preserve names of original test cases\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\n\ntest_shape(\"\", x)  # preserve names of original test cases\n\ntest_shape(\"_start_1\", x, start=1)\n\ntest_shape(\"_end_1\", x, end=1)\n\ntest_shape(\"_start_negative_1\", x, start=-1)\n\ntest_shape(\"_end_negative_1\", x, end=-1)\n\ntest_shape(\"_start_1_end_negative_1\", x, start=1, end=-1)\n\ntest_shape(\"_start_1_end_2\", x, start=1, end=2)\n\ntest_shape(\"_clip_start\", x, start=-10)\n\ntest_shape(\"_clip_end\", x, end=10)"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Gather",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Shape_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Shape_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Shape_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Shape",
            "module": "ai.onnx",
            "version": 13,
            "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "shape",
                "type": "T1",
                "description": "Shape of the input tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Input tensor can be of arbitrary type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain output to int64 tensor.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "shape",
                "code": "x = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n).astype(np.float32)\ntest_shape(\"_example\", x)  # preserve names of original test cases\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\n\ntest_shape(\"\", x)  # preserve names of original test cases\n\ntest_shape(\"_start_1\", x, start=1)\n\ntest_shape(\"_end_1\", x, end=1)\n\ntest_shape(\"_start_negative_1\", x, start=-1)\n\ntest_shape(\"_end_negative_1\", x, end=-1)\n\ntest_shape(\"_start_1_end_negative_1\", x, start=1, end=-1)\n\ntest_shape(\"_start_1_end_2\", x, start=1, end=2)\n\ntest_shape(\"_clip_start\", x, start=-10)\n\ntest_shape(\"_clip_end\", x, end=10)"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Gather_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Shape_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Constant_2_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/qkv_proj/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_538",
                  "initializer": {
                    "name": "onnx::MatMul_538",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          1536
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/qkv_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/qkv_proj/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.1.pre_norm_mha.1.qkv_proj.bias",
                  "initializer": {
                    "name": "transformer.1.pre_norm_mha.1.qkv_proj.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          1536
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        1536
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/qkv_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/qkv_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Unsqueeze",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_145",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Unsqueeze_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Unsqueeze_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_147",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Unsqueeze_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Concat",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "inputs",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Unsqueeze_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.1/pre_norm_mha.1/Unsqueeze_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.1/pre_norm_mha.1/Constant_3_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 3,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.1/pre_norm_mha.1/Constant_4_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 8,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.1/pre_norm_mha.1/Constant_5_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 255,
                      "1": 255,
                      "2": 255,
                      "3": 255,
                      "4": 255,
                      "5": 255,
                      "6": 255,
                      "7": 255
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "concat_result",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Concat_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        5
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Concat",
            "module": "ai.onnx",
            "version": 13,
            "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": true,
                "description": "Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs).."
              }
            ],
            "inputs": [
              {
                "name": "inputs",
                "type": "T",
                "list": true,
                "description": "List of tensors for concatenation"
              }
            ],
            "min_input": 1,
            "max_input": 2147483647,
            "outputs": [
              {
                "name": "concat_result",
                "type": "T",
                "description": "Concatenated tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - &#8734;",
            "type_constraints": [
              {
                "description": "Constrain output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "concat",
                "code": "test_cases: dict[str, Sequence[Any]] = {\n    \"1d\": ([1, 2], [3, 4]),\n    \"2d\": ([[1, 2], [3, 4]], [[5, 6], [7, 8]]),\n    \"3d\": (\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        [[[9, 10], [11, 12]], [[13, 14], [15, 16]]],\n    ),\n}\n\nfor test_case, values_ in test_cases.items():\n    values = [np.asarray(v, dtype=np.float32) for v in values_]\n    for i in range(len(values[0].shape)):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_\" + str(i),\n        )\n\n    for i in range(-len(values[0].shape), 0):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_negative_\" + str(abs(i)),\n        )"
              }
            ],
            "category": "Tensor"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Reshape",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/qkv_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        1536
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Concat_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        5
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reshaped",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Reshape_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__11",
                        "unk__12",
                        "unk__13",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "allowzero",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Reshape",
            "module": "ai.onnx",
            "version": 14,
            "description": "Reshape the input tensor similar to numpy.reshape.\nFirst input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\nAt most one dimension of the new shape can be -1. In this case, the value is\ninferred from the size of the tensor and the remaining dimensions. A dimension\ncould also be 0, in which case the actual dimension value is unchanged (i.e. taken\nfrom the input tensor). If 'allowzero' is set, and the new shape includes 0, the\ndimension will be set explicitly to zero (i.e. not taken from input tensor).\nShape (second input) could be an empty shape, which means converting to a scalar.\nThe input tensor's shape and the output tensor's shape are required to have the same number of elements.\n\nIf the attribute 'allowzero' is set, it is invalid for the specified shape to\ncontain both a zero value and -1, as the value of the dimension corresponding\nto -1 cannot be determined uniquely.\n",
            "attributes": [
              {
                "name": "allowzero",
                "type": "int64",
                "required": false,
                "description": "(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              },
              {
                "name": "shape",
                "type": "tensor(int64)",
                "description": "Specified shape for output."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "reshaped",
                "type": "T",
                "description": "Reshaped data."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "allowzero",
                "code": "original_shape = [0, 3, 4]\ntest_cases = {\n    \"allowzero_reordered\": np.array([3, 4, 0], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n        allowzero=1,  # if allowzero=1, final shape = (3, 4, 0)\n        # if allowzero=0, final shape = (3, 4, 4)\n    )\n\n    reshaped = reshape_reference_implementation(data, shape, allowzero=1)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              },
              {
                "summary": "reshape",
                "code": "original_shape = [2, 3, 4]\ntest_cases = {\n    \"reordered_all_dims\": np.array([4, 2, 3], dtype=np.int64),\n    \"reordered_last_dims\": np.array([2, 4, 3], dtype=np.int64),\n    \"reduced_dims\": np.array([2, 12], dtype=np.int64),\n    \"extended_dims\": np.array([2, 3, 2, 2], dtype=np.int64),\n    \"one_dim\": np.array([24], dtype=np.int64),\n    \"negative_dim\": np.array([2, -1, 2], dtype=np.int64),\n    \"negative_extended_dims\": np.array([-1, 2, 3, 4], dtype=np.int64),\n    \"zero_dim\": np.array([2, 0, 4, 1], dtype=np.int64),\n    \"zero_and_negative_dim\": np.array([2, 0, 1, -1], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n    )\n\n    reshaped = reshape_reference_implementation(data, shape)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              }
            ],
            "category": "Shape"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Transpose",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Reshape_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__11",
                        "unk__12",
                        "unk__13",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__12",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "3",
                  "2",
                  "1",
                  "4"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Gather_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__12",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Gather_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Gather_3",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__12",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Gather_3_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Gather_4",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__12",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.0/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Gather_4_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Gather_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Constant_6_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Transpose_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Gather_3_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Transpose_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__14",
                        "unk__11"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "1",
                  "3",
                  "2"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Transpose_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__14",
                        "unk__11"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__11"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__11"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__11"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/softmax/Softmax",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__11"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/softmax/Softmax_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__11"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "-1"
              }
            }
          ],
          "type": {
            "name": "Softmax",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator computes the normalized exponential values for the given input:\n\n Softmax(input, axis) = Exp(input) / ReduceSum(Exp(input), axis=axis, keepdims=1) \n\nThe \"axis\" attribute indicates the dimension along which Softmax\nwill be performed. The output tensor has the same shape\nand contains the Softmax values of the corresponding input.\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "default": -1,
                "description": "\nDescribes the dimension Softmax will be performed on.\nNegative value means counting dimensions\nfrom the back. Accepted range is [-r, r-1] where r = rank(input).\n"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T",
                "description": "The input tensor of rank >= axis."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "The output values with the same shape as the input tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "softmax",
                "code": "node = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\nx = np.array([[-1, 0, 1]]).astype(np.float32)\n# expected output [[0.09003058, 0.24472848, 0.66524094]]\ny = softmax(x, axis=1)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_example\")"
              },
              {
                "summary": "softmax_axis",
                "code": "x = np.array([[0, 1, 2, 3], [10000, 10001, 10002, 10003]]).astype(np.float32)\n# expected output\n# [[0.032058604 0.08714432  0.23688284  0.6439143  ]\n# [0.032058604 0.08714432  0.23688284  0.6439143  ]]\ny = softmax(x)\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_large_number\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ny = softmax(x, axis=0)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_axis_0\")\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ny = softmax(x, axis=1)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_axis_1\")\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=2,\n)\ny = softmax(x, axis=2)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_axis_2\")\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=-1,\n)\ny = softmax(x, axis=-1)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_negative_axis\")\n\n# default axis is -1\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_default_axis\")"
              }
            ],
            "category": "Activation"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/softmax/Softmax_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__11"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__11"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/MatMul_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__11"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Gather_4_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/MatMul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Transpose_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/MatMul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__13",
                        "unk__11",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Transpose_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__11",
                        "unk__13",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "2",
                  "1",
                  "3"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Unsqueeze_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_170",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Unsqueeze_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Unsqueeze_3",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_172",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Unsqueeze_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Concat_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "inputs",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Unsqueeze_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.1/pre_norm_mha.1/Unsqueeze_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.1/pre_norm_mha.1/Constant_7_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 255,
                      "1": 255,
                      "2": 255,
                      "3": 255,
                      "4": 255,
                      "5": 255,
                      "6": 255,
                      "7": 255
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "concat_result",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Concat_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Concat",
            "module": "ai.onnx",
            "version": 13,
            "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": true,
                "description": "Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs).."
              }
            ],
            "inputs": [
              {
                "name": "inputs",
                "type": "T",
                "list": true,
                "description": "List of tensors for concatenation"
              }
            ],
            "min_input": 1,
            "max_input": 2147483647,
            "outputs": [
              {
                "name": "concat_result",
                "type": "T",
                "description": "Concatenated tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - &#8734;",
            "type_constraints": [
              {
                "description": "Constrain output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "concat",
                "code": "test_cases: dict[str, Sequence[Any]] = {\n    \"1d\": ([1, 2], [3, 4]),\n    \"2d\": ([[1, 2], [3, 4]], [[5, 6], [7, 8]]),\n    \"3d\": (\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        [[[9, 10], [11, 12]], [[13, 14], [15, 16]]],\n    ),\n}\n\nfor test_case, values_ in test_cases.items():\n    values = [np.asarray(v, dtype=np.float32) for v in values_]\n    for i in range(len(values[0].shape)):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_\" + str(i),\n        )\n\n    for i in range(-len(values[0].shape), 0):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_negative_\" + str(abs(i)),\n        )"
              }
            ],
            "category": "Tensor"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/Reshape_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Transpose_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__10",
                        "unk__11",
                        "unk__13",
                        "unk__14"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Concat_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reshaped",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Reshape_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__15",
                        "unk__16",
                        "unk__17"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "allowzero",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Reshape",
            "module": "ai.onnx",
            "version": 14,
            "description": "Reshape the input tensor similar to numpy.reshape.\nFirst input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\nAt most one dimension of the new shape can be -1. In this case, the value is\ninferred from the size of the tensor and the remaining dimensions. A dimension\ncould also be 0, in which case the actual dimension value is unchanged (i.e. taken\nfrom the input tensor). If 'allowzero' is set, and the new shape includes 0, the\ndimension will be set explicitly to zero (i.e. not taken from input tensor).\nShape (second input) could be an empty shape, which means converting to a scalar.\nThe input tensor's shape and the output tensor's shape are required to have the same number of elements.\n\nIf the attribute 'allowzero' is set, it is invalid for the specified shape to\ncontain both a zero value and -1, as the value of the dimension corresponding\nto -1 cannot be determined uniquely.\n",
            "attributes": [
              {
                "name": "allowzero",
                "type": "int64",
                "required": false,
                "description": "(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              },
              {
                "name": "shape",
                "type": "tensor(int64)",
                "description": "Specified shape for output."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "reshaped",
                "type": "T",
                "description": "Reshaped data."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "allowzero",
                "code": "original_shape = [0, 3, 4]\ntest_cases = {\n    \"allowzero_reordered\": np.array([3, 4, 0], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n        allowzero=1,  # if allowzero=1, final shape = (3, 4, 0)\n        # if allowzero=0, final shape = (3, 4, 4)\n    )\n\n    reshaped = reshape_reference_implementation(data, shape, allowzero=1)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              },
              {
                "summary": "reshape",
                "code": "original_shape = [2, 3, 4]\ntest_cases = {\n    \"reordered_all_dims\": np.array([4, 2, 3], dtype=np.int64),\n    \"reordered_last_dims\": np.array([2, 4, 3], dtype=np.int64),\n    \"reduced_dims\": np.array([2, 12], dtype=np.int64),\n    \"extended_dims\": np.array([2, 3, 2, 2], dtype=np.int64),\n    \"one_dim\": np.array([24], dtype=np.int64),\n    \"negative_dim\": np.array([2, -1, 2], dtype=np.int64),\n    \"negative_extended_dims\": np.array([-1, 2, 3, 4], dtype=np.int64),\n    \"zero_dim\": np.array([2, 0, 4, 1], dtype=np.int64),\n    \"zero_and_negative_dim\": np.array([2, 0, 1, -1], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n    )\n\n    reshaped = reshape_reference_implementation(data, shape)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              }
            ],
            "category": "Shape"
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/out_proj/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Reshape_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__15",
                        "unk__16",
                        "unk__17"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_543",
                  "initializer": {
                    "name": "onnx::MatMul_543",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/out_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__15",
                        "unk__16",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_mha.1/out_proj/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.1.pre_norm_mha.1.out_proj.bias",
                  "initializer": {
                    "name": "transformer.1.pre_norm_mha.1.out_proj.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/out_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__15",
                        "unk__16",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/out_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__15",
                        "unk__16",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/out_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__15",
                        "unk__16",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__3",
                        "unk__9",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/ReduceMean",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Sub",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sub",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary subtraction (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sub",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([3, 2, 1]).astype(np.float32)\nz = x - y  # expected output [-2., 0., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint32\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint64)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint64\")"
              },
              {
                "summary": "sub_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Pow",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Z",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Pow",
            "module": "ai.onnx",
            "version": 13,
            "description": "Pow takes input data (Tensor<T>) and exponent Tensor, and\nproduces one output data (Tensor<T>) where the function `f(x) = x^exponent`,\nis applied to the data tensor elementwise.\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "First operand, base of the exponent."
              },
              {
                "name": "Y",
                "type": "T1",
                "description": "Second operand, power of the exponent."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Z",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input X and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain input Y types to float/int tensors.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "pow",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_example\")\n\nx = np.arange(60).reshape(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow\")"
              },
              {
                "summary": "pow_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array(2).astype(np.float32)\nz = pow(x, y)  # expected output [1., 4., 9.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_scalar\")\n\nnode = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\nx = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\ny = np.array([1, 2, 3]).astype(np.float32)\n# expected output [[1, 4, 27], [4, 25, 216]]\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_array\")"
              },
              {
                "summary": "types",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int32\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint64\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint32\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_int32\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Sqrt",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sqrt",
            "module": "ai.onnx",
            "version": 13,
            "description": "Square root takes one input data (Tensor<T>) and produces one output data\n(Tensor<T>) where the square root is, y = x^0.5, is applied to\nthe tensor elementwise. If x is negative, then it will return NaN.\n",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sqrt",
                "code": "node = onnx.helper.make_node(\n    \"Sqrt\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.array([1, 4, 9]).astype(np.float32)\ny = np.sqrt(x)  # expected output [1., 2., 3.]\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt_example\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\ny = np.sqrt(x)\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.1.pre_norm_ffn.0.weight",
                  "initializer": {
                    "name": "transformer.1.pre_norm_ffn.0.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.1.pre_norm_ffn.0.bias",
                  "initializer": {
                    "name": "transformer.1.pre_norm_ffn.0.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.1/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_544",
                  "initializer": {
                    "name": "onnx::MatMul_544",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          2048
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.1/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.1.pre_norm_ffn.1.bias",
                  "initializer": {
                    "name": "transformer.1.pre_norm_ffn.1.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          2048
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Erf",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Erf_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Erf",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the error function of the given input tensor element-wise.\n",
            "inputs": [
              {
                "name": "input",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "The error function of the input tensor computed element-wise. It has the same shape and type of the input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "erf",
                "code": "node = onnx.helper.make_node(\n    \"Erf\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.random.randn(1, 3, 32, 32).astype(np.float32)\ny = np.vectorize(math.erf)(x).astype(np.float32)\nexpect(node, inputs=[x], outputs=[y], name=\"test_erf\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Erf_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Mul_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Constant_2_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Mul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.4/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.2/Mul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_545",
                  "initializer": {
                    "name": "onnx::MatMul_545",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          2048,
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        2048,
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.4/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.4/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.1.pre_norm_ffn.4.bias",
                  "initializer": {
                    "name": "transformer.1.pre_norm_ffn.4.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.4/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.4/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.1/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_ffn/pre_norm_ffn.4/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.1/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.0/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.1/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.0/ReduceMean",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.0/Sub",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sub",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary subtraction (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sub",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([3, 2, 1]).astype(np.float32)\nz = x - y  # expected output [-2., 0., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint32\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint64)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint64\")"
              },
              {
                "summary": "sub_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.0/Pow",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Z",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Pow",
            "module": "ai.onnx",
            "version": 13,
            "description": "Pow takes input data (Tensor<T>) and exponent Tensor, and\nproduces one output data (Tensor<T>) where the function `f(x) = x^exponent`,\nis applied to the data tensor elementwise.\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "First operand, base of the exponent."
              },
              {
                "name": "Y",
                "type": "T1",
                "description": "Second operand, power of the exponent."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Z",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input X and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain input Y types to float/int tensors.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "pow",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_example\")\n\nx = np.arange(60).reshape(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow\")"
              },
              {
                "summary": "pow_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array(2).astype(np.float32)\nz = pow(x, y)  # expected output [1., 4., 9.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_scalar\")\n\nnode = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\nx = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\ny = np.array([1, 2, 3]).astype(np.float32)\n# expected output [[1, 4, 27], [4, 25, 216]]\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_array\")"
              },
              {
                "summary": "types",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int32\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint64\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint32\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_int32\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.0/ReduceMean_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.0/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.0/Sqrt",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sqrt",
            "module": "ai.onnx",
            "version": 13,
            "description": "Square root takes one input data (Tensor<T>) and produces one output data\n(Tensor<T>) where the square root is, y = x^0.5, is applied to\nthe tensor elementwise. If x is negative, then it will return NaN.\n",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sqrt",
                "code": "node = onnx.helper.make_node(\n    \"Sqrt\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.array([1, 4, 9]).astype(np.float32)\ny = np.sqrt(x)  # expected output [1., 2., 3.]\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt_example\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\ny = np.sqrt(x)\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.0/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.0/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.2.pre_norm_mha.0.weight",
                  "initializer": {
                    "name": "transformer.2.pre_norm_mha.0.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.0/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.2.pre_norm_mha.0.bias",
                  "initializer": {
                    "name": "transformer.2.pre_norm_mha.0.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.0/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Shape",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Shape_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Shape",
            "module": "ai.onnx",
            "version": 13,
            "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "shape",
                "type": "T1",
                "description": "Shape of the input tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Input tensor can be of arbitrary type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain output to int64 tensor.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "shape",
                "code": "x = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n).astype(np.float32)\ntest_shape(\"_example\", x)  # preserve names of original test cases\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\n\ntest_shape(\"\", x)  # preserve names of original test cases\n\ntest_shape(\"_start_1\", x, start=1)\n\ntest_shape(\"_end_1\", x, end=1)\n\ntest_shape(\"_start_negative_1\", x, start=-1)\n\ntest_shape(\"_end_negative_1\", x, end=-1)\n\ntest_shape(\"_start_1_end_negative_1\", x, start=1, end=-1)\n\ntest_shape(\"_start_1_end_2\", x, start=1, end=2)\n\ntest_shape(\"_clip_start\", x, start=-10)\n\ntest_shape(\"_clip_end\", x, end=10)"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Gather",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Shape_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Shape_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Shape_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Shape",
            "module": "ai.onnx",
            "version": 13,
            "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "shape",
                "type": "T1",
                "description": "Shape of the input tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Input tensor can be of arbitrary type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain output to int64 tensor.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "shape",
                "code": "x = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n).astype(np.float32)\ntest_shape(\"_example\", x)  # preserve names of original test cases\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\n\ntest_shape(\"\", x)  # preserve names of original test cases\n\ntest_shape(\"_start_1\", x, start=1)\n\ntest_shape(\"_end_1\", x, end=1)\n\ntest_shape(\"_start_negative_1\", x, start=-1)\n\ntest_shape(\"_end_negative_1\", x, end=-1)\n\ntest_shape(\"_start_1_end_negative_1\", x, start=1, end=-1)\n\ntest_shape(\"_start_1_end_2\", x, start=1, end=2)\n\ntest_shape(\"_clip_start\", x, start=-10)\n\ntest_shape(\"_clip_end\", x, end=10)"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Gather_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Shape_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/qkv_proj/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_546",
                  "initializer": {
                    "name": "onnx::MatMul_546",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          1536
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/qkv_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/qkv_proj/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.2.pre_norm_mha.1.qkv_proj.bias",
                  "initializer": {
                    "name": "transformer.2.pre_norm_mha.1.qkv_proj.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          1536
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        1536
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/qkv_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/qkv_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Unsqueeze",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_232",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Unsqueeze_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Unsqueeze_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_234",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Unsqueeze_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Concat",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "inputs",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Unsqueeze_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.2/pre_norm_mha.1/Unsqueeze_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.2/pre_norm_mha.1/Constant_2_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 3,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.2/pre_norm_mha.1/Constant_3_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 8,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.2/pre_norm_mha.1/Constant_4_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 255,
                      "1": 255,
                      "2": 255,
                      "3": 255,
                      "4": 255,
                      "5": 255,
                      "6": 255,
                      "7": 255
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "concat_result",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Concat_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        5
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Concat",
            "module": "ai.onnx",
            "version": 13,
            "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": true,
                "description": "Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs).."
              }
            ],
            "inputs": [
              {
                "name": "inputs",
                "type": "T",
                "list": true,
                "description": "List of tensors for concatenation"
              }
            ],
            "min_input": 1,
            "max_input": 2147483647,
            "outputs": [
              {
                "name": "concat_result",
                "type": "T",
                "description": "Concatenated tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - &#8734;",
            "type_constraints": [
              {
                "description": "Constrain output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "concat",
                "code": "test_cases: dict[str, Sequence[Any]] = {\n    \"1d\": ([1, 2], [3, 4]),\n    \"2d\": ([[1, 2], [3, 4]], [[5, 6], [7, 8]]),\n    \"3d\": (\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        [[[9, 10], [11, 12]], [[13, 14], [15, 16]]],\n    ),\n}\n\nfor test_case, values_ in test_cases.items():\n    values = [np.asarray(v, dtype=np.float32) for v in values_]\n    for i in range(len(values[0].shape)):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_\" + str(i),\n        )\n\n    for i in range(-len(values[0].shape), 0):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_negative_\" + str(abs(i)),\n        )"
              }
            ],
            "category": "Tensor"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Reshape",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/qkv_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        1536
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Concat_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        5
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reshaped",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Reshape_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__21",
                        "unk__22",
                        "unk__23",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "allowzero",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Reshape",
            "module": "ai.onnx",
            "version": 14,
            "description": "Reshape the input tensor similar to numpy.reshape.\nFirst input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\nAt most one dimension of the new shape can be -1. In this case, the value is\ninferred from the size of the tensor and the remaining dimensions. A dimension\ncould also be 0, in which case the actual dimension value is unchanged (i.e. taken\nfrom the input tensor). If 'allowzero' is set, and the new shape includes 0, the\ndimension will be set explicitly to zero (i.e. not taken from input tensor).\nShape (second input) could be an empty shape, which means converting to a scalar.\nThe input tensor's shape and the output tensor's shape are required to have the same number of elements.\n\nIf the attribute 'allowzero' is set, it is invalid for the specified shape to\ncontain both a zero value and -1, as the value of the dimension corresponding\nto -1 cannot be determined uniquely.\n",
            "attributes": [
              {
                "name": "allowzero",
                "type": "int64",
                "required": false,
                "description": "(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              },
              {
                "name": "shape",
                "type": "tensor(int64)",
                "description": "Specified shape for output."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "reshaped",
                "type": "T",
                "description": "Reshaped data."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "allowzero",
                "code": "original_shape = [0, 3, 4]\ntest_cases = {\n    \"allowzero_reordered\": np.array([3, 4, 0], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n        allowzero=1,  # if allowzero=1, final shape = (3, 4, 0)\n        # if allowzero=0, final shape = (3, 4, 4)\n    )\n\n    reshaped = reshape_reference_implementation(data, shape, allowzero=1)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              },
              {
                "summary": "reshape",
                "code": "original_shape = [2, 3, 4]\ntest_cases = {\n    \"reordered_all_dims\": np.array([4, 2, 3], dtype=np.int64),\n    \"reordered_last_dims\": np.array([2, 4, 3], dtype=np.int64),\n    \"reduced_dims\": np.array([2, 12], dtype=np.int64),\n    \"extended_dims\": np.array([2, 3, 2, 2], dtype=np.int64),\n    \"one_dim\": np.array([24], dtype=np.int64),\n    \"negative_dim\": np.array([2, -1, 2], dtype=np.int64),\n    \"negative_extended_dims\": np.array([-1, 2, 3, 4], dtype=np.int64),\n    \"zero_dim\": np.array([2, 0, 4, 1], dtype=np.int64),\n    \"zero_and_negative_dim\": np.array([2, 0, 1, -1], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n    )\n\n    reshaped = reshape_reference_implementation(data, shape)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              }
            ],
            "category": "Shape"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Transpose",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Reshape_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__21",
                        "unk__22",
                        "unk__23",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__22",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "3",
                  "2",
                  "1",
                  "4"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Gather_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__22",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Gather_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Gather_3",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__22",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Gather_3_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Gather_4",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__22",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.0/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Gather_4_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Gather_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Constant_5_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Transpose_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Gather_3_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Transpose_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__24",
                        "unk__21"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "1",
                  "3",
                  "2"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Transpose_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__24",
                        "unk__21"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__21"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__21"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__21"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/softmax/Softmax",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__21"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/softmax/Softmax_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__21"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "-1"
              }
            }
          ],
          "type": {
            "name": "Softmax",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator computes the normalized exponential values for the given input:\n\n Softmax(input, axis) = Exp(input) / ReduceSum(Exp(input), axis=axis, keepdims=1) \n\nThe \"axis\" attribute indicates the dimension along which Softmax\nwill be performed. The output tensor has the same shape\nand contains the Softmax values of the corresponding input.\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "default": -1,
                "description": "\nDescribes the dimension Softmax will be performed on.\nNegative value means counting dimensions\nfrom the back. Accepted range is [-r, r-1] where r = rank(input).\n"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T",
                "description": "The input tensor of rank >= axis."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "The output values with the same shape as the input tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "softmax",
                "code": "node = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\nx = np.array([[-1, 0, 1]]).astype(np.float32)\n# expected output [[0.09003058, 0.24472848, 0.66524094]]\ny = softmax(x, axis=1)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_example\")"
              },
              {
                "summary": "softmax_axis",
                "code": "x = np.array([[0, 1, 2, 3], [10000, 10001, 10002, 10003]]).astype(np.float32)\n# expected output\n# [[0.032058604 0.08714432  0.23688284  0.6439143  ]\n# [0.032058604 0.08714432  0.23688284  0.6439143  ]]\ny = softmax(x)\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_large_number\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ny = softmax(x, axis=0)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_axis_0\")\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ny = softmax(x, axis=1)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_axis_1\")\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=2,\n)\ny = softmax(x, axis=2)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_axis_2\")\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=-1,\n)\ny = softmax(x, axis=-1)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_negative_axis\")\n\n# default axis is -1\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_default_axis\")"
              }
            ],
            "category": "Activation"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/softmax/Softmax_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__21"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__21"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/MatMul_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__21"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Gather_4_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/MatMul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Transpose_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/MatMul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__23",
                        "unk__21",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Transpose_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__21",
                        "unk__23",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "2",
                  "1",
                  "3"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Unsqueeze_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_257",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Unsqueeze_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Unsqueeze_3",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_259",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Unsqueeze_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Concat_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "inputs",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Unsqueeze_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.2/pre_norm_mha.1/Unsqueeze_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.2/pre_norm_mha.1/Constant_6_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 255,
                      "1": 255,
                      "2": 255,
                      "3": 255,
                      "4": 255,
                      "5": 255,
                      "6": 255,
                      "7": 255
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "concat_result",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Concat_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Concat",
            "module": "ai.onnx",
            "version": 13,
            "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": true,
                "description": "Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs).."
              }
            ],
            "inputs": [
              {
                "name": "inputs",
                "type": "T",
                "list": true,
                "description": "List of tensors for concatenation"
              }
            ],
            "min_input": 1,
            "max_input": 2147483647,
            "outputs": [
              {
                "name": "concat_result",
                "type": "T",
                "description": "Concatenated tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - &#8734;",
            "type_constraints": [
              {
                "description": "Constrain output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "concat",
                "code": "test_cases: dict[str, Sequence[Any]] = {\n    \"1d\": ([1, 2], [3, 4]),\n    \"2d\": ([[1, 2], [3, 4]], [[5, 6], [7, 8]]),\n    \"3d\": (\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        [[[9, 10], [11, 12]], [[13, 14], [15, 16]]],\n    ),\n}\n\nfor test_case, values_ in test_cases.items():\n    values = [np.asarray(v, dtype=np.float32) for v in values_]\n    for i in range(len(values[0].shape)):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_\" + str(i),\n        )\n\n    for i in range(-len(values[0].shape), 0):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_negative_\" + str(abs(i)),\n        )"
              }
            ],
            "category": "Tensor"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/Reshape_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Transpose_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__20",
                        "unk__21",
                        "unk__23",
                        "unk__24"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Concat_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reshaped",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Reshape_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__25",
                        "unk__26",
                        "unk__27"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "allowzero",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Reshape",
            "module": "ai.onnx",
            "version": 14,
            "description": "Reshape the input tensor similar to numpy.reshape.\nFirst input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\nAt most one dimension of the new shape can be -1. In this case, the value is\ninferred from the size of the tensor and the remaining dimensions. A dimension\ncould also be 0, in which case the actual dimension value is unchanged (i.e. taken\nfrom the input tensor). If 'allowzero' is set, and the new shape includes 0, the\ndimension will be set explicitly to zero (i.e. not taken from input tensor).\nShape (second input) could be an empty shape, which means converting to a scalar.\nThe input tensor's shape and the output tensor's shape are required to have the same number of elements.\n\nIf the attribute 'allowzero' is set, it is invalid for the specified shape to\ncontain both a zero value and -1, as the value of the dimension corresponding\nto -1 cannot be determined uniquely.\n",
            "attributes": [
              {
                "name": "allowzero",
                "type": "int64",
                "required": false,
                "description": "(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              },
              {
                "name": "shape",
                "type": "tensor(int64)",
                "description": "Specified shape for output."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "reshaped",
                "type": "T",
                "description": "Reshaped data."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "allowzero",
                "code": "original_shape = [0, 3, 4]\ntest_cases = {\n    \"allowzero_reordered\": np.array([3, 4, 0], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n        allowzero=1,  # if allowzero=1, final shape = (3, 4, 0)\n        # if allowzero=0, final shape = (3, 4, 4)\n    )\n\n    reshaped = reshape_reference_implementation(data, shape, allowzero=1)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              },
              {
                "summary": "reshape",
                "code": "original_shape = [2, 3, 4]\ntest_cases = {\n    \"reordered_all_dims\": np.array([4, 2, 3], dtype=np.int64),\n    \"reordered_last_dims\": np.array([2, 4, 3], dtype=np.int64),\n    \"reduced_dims\": np.array([2, 12], dtype=np.int64),\n    \"extended_dims\": np.array([2, 3, 2, 2], dtype=np.int64),\n    \"one_dim\": np.array([24], dtype=np.int64),\n    \"negative_dim\": np.array([2, -1, 2], dtype=np.int64),\n    \"negative_extended_dims\": np.array([-1, 2, 3, 4], dtype=np.int64),\n    \"zero_dim\": np.array([2, 0, 4, 1], dtype=np.int64),\n    \"zero_and_negative_dim\": np.array([2, 0, 1, -1], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n    )\n\n    reshaped = reshape_reference_implementation(data, shape)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              }
            ],
            "category": "Shape"
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/out_proj/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/Reshape_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__25",
                        "unk__26",
                        "unk__27"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_551",
                  "initializer": {
                    "name": "onnx::MatMul_551",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/out_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__25",
                        "unk__26",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_mha.1/out_proj/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.2.pre_norm_mha.1.out_proj.bias",
                  "initializer": {
                    "name": "transformer.2.pre_norm_mha.1.out_proj.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/out_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__25",
                        "unk__26",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/out_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__25",
                        "unk__26",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.1/out_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__25",
                        "unk__26",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__18",
                        "unk__19",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.2/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/ReduceMean",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Sub",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sub",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary subtraction (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sub",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([3, 2, 1]).astype(np.float32)\nz = x - y  # expected output [-2., 0., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint32\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint64)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint64\")"
              },
              {
                "summary": "sub_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Pow",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Z",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Pow",
            "module": "ai.onnx",
            "version": 13,
            "description": "Pow takes input data (Tensor<T>) and exponent Tensor, and\nproduces one output data (Tensor<T>) where the function `f(x) = x^exponent`,\nis applied to the data tensor elementwise.\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "First operand, base of the exponent."
              },
              {
                "name": "Y",
                "type": "T1",
                "description": "Second operand, power of the exponent."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Z",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input X and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain input Y types to float/int tensors.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "pow",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_example\")\n\nx = np.arange(60).reshape(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow\")"
              },
              {
                "summary": "pow_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array(2).astype(np.float32)\nz = pow(x, y)  # expected output [1., 4., 9.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_scalar\")\n\nnode = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\nx = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\ny = np.array([1, 2, 3]).astype(np.float32)\n# expected output [[1, 4, 27], [4, 25, 216]]\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_array\")"
              },
              {
                "summary": "types",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int32\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint64\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint32\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_int32\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Sqrt",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sqrt",
            "module": "ai.onnx",
            "version": 13,
            "description": "Square root takes one input data (Tensor<T>) and produces one output data\n(Tensor<T>) where the square root is, y = x^0.5, is applied to\nthe tensor elementwise. If x is negative, then it will return NaN.\n",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sqrt",
                "code": "node = onnx.helper.make_node(\n    \"Sqrt\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.array([1, 4, 9]).astype(np.float32)\ny = np.sqrt(x)  # expected output [1., 2., 3.]\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt_example\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\ny = np.sqrt(x)\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.2.pre_norm_ffn.0.weight",
                  "initializer": {
                    "name": "transformer.2.pre_norm_ffn.0.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.2.pre_norm_ffn.0.bias",
                  "initializer": {
                    "name": "transformer.2.pre_norm_ffn.0.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.1/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_552",
                  "initializer": {
                    "name": "onnx::MatMul_552",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          2048
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.1/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.2.pre_norm_ffn.1.bias",
                  "initializer": {
                    "name": "transformer.2.pre_norm_ffn.1.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          2048
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Erf",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Erf_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Erf",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the error function of the given input tensor element-wise.\n",
            "inputs": [
              {
                "name": "input",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "The error function of the input tensor computed element-wise. It has the same shape and type of the input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "erf",
                "code": "node = onnx.helper.make_node(\n    \"Erf\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.random.randn(1, 3, 32, 32).astype(np.float32)\ny = np.vectorize(math.erf)(x).astype(np.float32)\nexpect(node, inputs=[x], outputs=[y], name=\"test_erf\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Erf_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Mul_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Constant_2_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Mul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.4/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.2/Mul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_553",
                  "initializer": {
                    "name": "onnx::MatMul_553",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          2048,
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        2048,
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.4/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.4/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.2.pre_norm_ffn.4.bias",
                  "initializer": {
                    "name": "transformer.2.pre_norm_ffn.4.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.4/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.4/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.2/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.2/pre_norm_ffn/pre_norm_ffn.4/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.2/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.0/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.2/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.0/ReduceMean",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.0/Sub",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sub",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary subtraction (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sub",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([3, 2, 1]).astype(np.float32)\nz = x - y  # expected output [-2., 0., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint32\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint64)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint64\")"
              },
              {
                "summary": "sub_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.0/Pow",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Z",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Pow",
            "module": "ai.onnx",
            "version": 13,
            "description": "Pow takes input data (Tensor<T>) and exponent Tensor, and\nproduces one output data (Tensor<T>) where the function `f(x) = x^exponent`,\nis applied to the data tensor elementwise.\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "First operand, base of the exponent."
              },
              {
                "name": "Y",
                "type": "T1",
                "description": "Second operand, power of the exponent."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Z",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input X and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain input Y types to float/int tensors.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "pow",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_example\")\n\nx = np.arange(60).reshape(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow\")"
              },
              {
                "summary": "pow_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array(2).astype(np.float32)\nz = pow(x, y)  # expected output [1., 4., 9.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_scalar\")\n\nnode = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\nx = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\ny = np.array([1, 2, 3]).astype(np.float32)\n# expected output [[1, 4, 27], [4, 25, 216]]\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_array\")"
              },
              {
                "summary": "types",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int32\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint64\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint32\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_int32\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.0/ReduceMean_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.0/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.0/Sqrt",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sqrt",
            "module": "ai.onnx",
            "version": 13,
            "description": "Square root takes one input data (Tensor<T>) and produces one output data\n(Tensor<T>) where the square root is, y = x^0.5, is applied to\nthe tensor elementwise. If x is negative, then it will return NaN.\n",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sqrt",
                "code": "node = onnx.helper.make_node(\n    \"Sqrt\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.array([1, 4, 9]).astype(np.float32)\ny = np.sqrt(x)  # expected output [1., 2., 3.]\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt_example\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\ny = np.sqrt(x)\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.0/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.0/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.3.pre_norm_mha.0.weight",
                  "initializer": {
                    "name": "transformer.3.pre_norm_mha.0.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.0/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.3.pre_norm_mha.0.bias",
                  "initializer": {
                    "name": "transformer.3.pre_norm_mha.0.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.0/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Shape",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Shape_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Shape",
            "module": "ai.onnx",
            "version": 13,
            "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "shape",
                "type": "T1",
                "description": "Shape of the input tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Input tensor can be of arbitrary type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain output to int64 tensor.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "shape",
                "code": "x = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n).astype(np.float32)\ntest_shape(\"_example\", x)  # preserve names of original test cases\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\n\ntest_shape(\"\", x)  # preserve names of original test cases\n\ntest_shape(\"_start_1\", x, start=1)\n\ntest_shape(\"_end_1\", x, end=1)\n\ntest_shape(\"_start_negative_1\", x, start=-1)\n\ntest_shape(\"_end_negative_1\", x, end=-1)\n\ntest_shape(\"_start_1_end_negative_1\", x, start=1, end=-1)\n\ntest_shape(\"_start_1_end_2\", x, start=1, end=2)\n\ntest_shape(\"_clip_start\", x, start=-10)\n\ntest_shape(\"_clip_end\", x, end=10)"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Gather",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Shape_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Shape_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Shape_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Shape",
            "module": "ai.onnx",
            "version": 13,
            "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "shape",
                "type": "T1",
                "description": "Shape of the input tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Input tensor can be of arbitrary type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain output to int64 tensor.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "shape",
                "code": "x = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n).astype(np.float32)\ntest_shape(\"_example\", x)  # preserve names of original test cases\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\n\ntest_shape(\"\", x)  # preserve names of original test cases\n\ntest_shape(\"_start_1\", x, start=1)\n\ntest_shape(\"_end_1\", x, end=1)\n\ntest_shape(\"_start_negative_1\", x, start=-1)\n\ntest_shape(\"_end_negative_1\", x, end=-1)\n\ntest_shape(\"_start_1_end_negative_1\", x, start=1, end=-1)\n\ntest_shape(\"_start_1_end_2\", x, start=1, end=2)\n\ntest_shape(\"_clip_start\", x, start=-10)\n\ntest_shape(\"_clip_end\", x, end=10)"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Gather_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Shape_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/qkv_proj/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_554",
                  "initializer": {
                    "name": "onnx::MatMul_554",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          1536
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/qkv_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/qkv_proj/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.3.pre_norm_mha.1.qkv_proj.bias",
                  "initializer": {
                    "name": "transformer.3.pre_norm_mha.1.qkv_proj.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          1536
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        1536
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/qkv_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/qkv_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Unsqueeze",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_319",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Unsqueeze_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Unsqueeze_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_321",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Unsqueeze_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Concat",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "inputs",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Unsqueeze_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.3/pre_norm_mha.1/Unsqueeze_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.3/pre_norm_mha.1/Constant_2_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 3,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.3/pre_norm_mha.1/Constant_3_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 8,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.3/pre_norm_mha.1/Constant_4_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 255,
                      "1": 255,
                      "2": 255,
                      "3": 255,
                      "4": 255,
                      "5": 255,
                      "6": 255,
                      "7": 255
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "concat_result",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Concat_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        5
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Concat",
            "module": "ai.onnx",
            "version": 13,
            "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": true,
                "description": "Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs).."
              }
            ],
            "inputs": [
              {
                "name": "inputs",
                "type": "T",
                "list": true,
                "description": "List of tensors for concatenation"
              }
            ],
            "min_input": 1,
            "max_input": 2147483647,
            "outputs": [
              {
                "name": "concat_result",
                "type": "T",
                "description": "Concatenated tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - &#8734;",
            "type_constraints": [
              {
                "description": "Constrain output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "concat",
                "code": "test_cases: dict[str, Sequence[Any]] = {\n    \"1d\": ([1, 2], [3, 4]),\n    \"2d\": ([[1, 2], [3, 4]], [[5, 6], [7, 8]]),\n    \"3d\": (\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        [[[9, 10], [11, 12]], [[13, 14], [15, 16]]],\n    ),\n}\n\nfor test_case, values_ in test_cases.items():\n    values = [np.asarray(v, dtype=np.float32) for v in values_]\n    for i in range(len(values[0].shape)):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_\" + str(i),\n        )\n\n    for i in range(-len(values[0].shape), 0):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_negative_\" + str(abs(i)),\n        )"
              }
            ],
            "category": "Tensor"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Reshape",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/qkv_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        1536
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Concat_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        5
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reshaped",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Reshape_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__31",
                        "unk__32",
                        "unk__33",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "allowzero",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Reshape",
            "module": "ai.onnx",
            "version": 14,
            "description": "Reshape the input tensor similar to numpy.reshape.\nFirst input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\nAt most one dimension of the new shape can be -1. In this case, the value is\ninferred from the size of the tensor and the remaining dimensions. A dimension\ncould also be 0, in which case the actual dimension value is unchanged (i.e. taken\nfrom the input tensor). If 'allowzero' is set, and the new shape includes 0, the\ndimension will be set explicitly to zero (i.e. not taken from input tensor).\nShape (second input) could be an empty shape, which means converting to a scalar.\nThe input tensor's shape and the output tensor's shape are required to have the same number of elements.\n\nIf the attribute 'allowzero' is set, it is invalid for the specified shape to\ncontain both a zero value and -1, as the value of the dimension corresponding\nto -1 cannot be determined uniquely.\n",
            "attributes": [
              {
                "name": "allowzero",
                "type": "int64",
                "required": false,
                "description": "(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              },
              {
                "name": "shape",
                "type": "tensor(int64)",
                "description": "Specified shape for output."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "reshaped",
                "type": "T",
                "description": "Reshaped data."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "allowzero",
                "code": "original_shape = [0, 3, 4]\ntest_cases = {\n    \"allowzero_reordered\": np.array([3, 4, 0], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n        allowzero=1,  # if allowzero=1, final shape = (3, 4, 0)\n        # if allowzero=0, final shape = (3, 4, 4)\n    )\n\n    reshaped = reshape_reference_implementation(data, shape, allowzero=1)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              },
              {
                "summary": "reshape",
                "code": "original_shape = [2, 3, 4]\ntest_cases = {\n    \"reordered_all_dims\": np.array([4, 2, 3], dtype=np.int64),\n    \"reordered_last_dims\": np.array([2, 4, 3], dtype=np.int64),\n    \"reduced_dims\": np.array([2, 12], dtype=np.int64),\n    \"extended_dims\": np.array([2, 3, 2, 2], dtype=np.int64),\n    \"one_dim\": np.array([24], dtype=np.int64),\n    \"negative_dim\": np.array([2, -1, 2], dtype=np.int64),\n    \"negative_extended_dims\": np.array([-1, 2, 3, 4], dtype=np.int64),\n    \"zero_dim\": np.array([2, 0, 4, 1], dtype=np.int64),\n    \"zero_and_negative_dim\": np.array([2, 0, 1, -1], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n    )\n\n    reshaped = reshape_reference_implementation(data, shape)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              }
            ],
            "category": "Shape"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Transpose",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Reshape_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__31",
                        "unk__32",
                        "unk__33",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__32",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "3",
                  "2",
                  "1",
                  "4"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Gather_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__32",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Gather_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Gather_3",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__32",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Gather_3_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Gather_4",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__32",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.0/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Gather_4_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Gather_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Constant_5_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Transpose_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Gather_3_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Transpose_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__34",
                        "unk__31"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "1",
                  "3",
                  "2"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Transpose_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__34",
                        "unk__31"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__31"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__31"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__31"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/softmax/Softmax",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__31"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/softmax/Softmax_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__31"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "-1"
              }
            }
          ],
          "type": {
            "name": "Softmax",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator computes the normalized exponential values for the given input:\n\n Softmax(input, axis) = Exp(input) / ReduceSum(Exp(input), axis=axis, keepdims=1) \n\nThe \"axis\" attribute indicates the dimension along which Softmax\nwill be performed. The output tensor has the same shape\nand contains the Softmax values of the corresponding input.\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "default": -1,
                "description": "\nDescribes the dimension Softmax will be performed on.\nNegative value means counting dimensions\nfrom the back. Accepted range is [-r, r-1] where r = rank(input).\n"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T",
                "description": "The input tensor of rank >= axis."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "The output values with the same shape as the input tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "softmax",
                "code": "node = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\nx = np.array([[-1, 0, 1]]).astype(np.float32)\n# expected output [[0.09003058, 0.24472848, 0.66524094]]\ny = softmax(x, axis=1)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_example\")"
              },
              {
                "summary": "softmax_axis",
                "code": "x = np.array([[0, 1, 2, 3], [10000, 10001, 10002, 10003]]).astype(np.float32)\n# expected output\n# [[0.032058604 0.08714432  0.23688284  0.6439143  ]\n# [0.032058604 0.08714432  0.23688284  0.6439143  ]]\ny = softmax(x)\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_large_number\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ny = softmax(x, axis=0)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_axis_0\")\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ny = softmax(x, axis=1)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_axis_1\")\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=2,\n)\ny = softmax(x, axis=2)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_axis_2\")\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=-1,\n)\ny = softmax(x, axis=-1)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_negative_axis\")\n\n# default axis is -1\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_default_axis\")"
              }
            ],
            "category": "Activation"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/softmax/Softmax_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__31"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__31"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/MatMul_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__31"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Gather_4_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/MatMul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Transpose_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/MatMul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__33",
                        "unk__31",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Transpose_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__31",
                        "unk__33",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "2",
                  "1",
                  "3"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Unsqueeze_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_344",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Unsqueeze_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Unsqueeze_3",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_346",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Unsqueeze_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Concat_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "inputs",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Unsqueeze_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.3/pre_norm_mha.1/Unsqueeze_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.3/pre_norm_mha.1/Constant_6_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 255,
                      "1": 255,
                      "2": 255,
                      "3": 255,
                      "4": 255,
                      "5": 255,
                      "6": 255,
                      "7": 255
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "concat_result",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Concat_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Concat",
            "module": "ai.onnx",
            "version": 13,
            "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": true,
                "description": "Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs).."
              }
            ],
            "inputs": [
              {
                "name": "inputs",
                "type": "T",
                "list": true,
                "description": "List of tensors for concatenation"
              }
            ],
            "min_input": 1,
            "max_input": 2147483647,
            "outputs": [
              {
                "name": "concat_result",
                "type": "T",
                "description": "Concatenated tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - &#8734;",
            "type_constraints": [
              {
                "description": "Constrain output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "concat",
                "code": "test_cases: dict[str, Sequence[Any]] = {\n    \"1d\": ([1, 2], [3, 4]),\n    \"2d\": ([[1, 2], [3, 4]], [[5, 6], [7, 8]]),\n    \"3d\": (\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        [[[9, 10], [11, 12]], [[13, 14], [15, 16]]],\n    ),\n}\n\nfor test_case, values_ in test_cases.items():\n    values = [np.asarray(v, dtype=np.float32) for v in values_]\n    for i in range(len(values[0].shape)):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_\" + str(i),\n        )\n\n    for i in range(-len(values[0].shape), 0):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_negative_\" + str(abs(i)),\n        )"
              }
            ],
            "category": "Tensor"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/Reshape_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Transpose_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__30",
                        "unk__31",
                        "unk__33",
                        "unk__34"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Concat_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reshaped",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Reshape_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__35",
                        "unk__36",
                        "unk__37"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "allowzero",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Reshape",
            "module": "ai.onnx",
            "version": 14,
            "description": "Reshape the input tensor similar to numpy.reshape.\nFirst input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\nAt most one dimension of the new shape can be -1. In this case, the value is\ninferred from the size of the tensor and the remaining dimensions. A dimension\ncould also be 0, in which case the actual dimension value is unchanged (i.e. taken\nfrom the input tensor). If 'allowzero' is set, and the new shape includes 0, the\ndimension will be set explicitly to zero (i.e. not taken from input tensor).\nShape (second input) could be an empty shape, which means converting to a scalar.\nThe input tensor's shape and the output tensor's shape are required to have the same number of elements.\n\nIf the attribute 'allowzero' is set, it is invalid for the specified shape to\ncontain both a zero value and -1, as the value of the dimension corresponding\nto -1 cannot be determined uniquely.\n",
            "attributes": [
              {
                "name": "allowzero",
                "type": "int64",
                "required": false,
                "description": "(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              },
              {
                "name": "shape",
                "type": "tensor(int64)",
                "description": "Specified shape for output."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "reshaped",
                "type": "T",
                "description": "Reshaped data."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "allowzero",
                "code": "original_shape = [0, 3, 4]\ntest_cases = {\n    \"allowzero_reordered\": np.array([3, 4, 0], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n        allowzero=1,  # if allowzero=1, final shape = (3, 4, 0)\n        # if allowzero=0, final shape = (3, 4, 4)\n    )\n\n    reshaped = reshape_reference_implementation(data, shape, allowzero=1)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              },
              {
                "summary": "reshape",
                "code": "original_shape = [2, 3, 4]\ntest_cases = {\n    \"reordered_all_dims\": np.array([4, 2, 3], dtype=np.int64),\n    \"reordered_last_dims\": np.array([2, 4, 3], dtype=np.int64),\n    \"reduced_dims\": np.array([2, 12], dtype=np.int64),\n    \"extended_dims\": np.array([2, 3, 2, 2], dtype=np.int64),\n    \"one_dim\": np.array([24], dtype=np.int64),\n    \"negative_dim\": np.array([2, -1, 2], dtype=np.int64),\n    \"negative_extended_dims\": np.array([-1, 2, 3, 4], dtype=np.int64),\n    \"zero_dim\": np.array([2, 0, 4, 1], dtype=np.int64),\n    \"zero_and_negative_dim\": np.array([2, 0, 1, -1], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n    )\n\n    reshaped = reshape_reference_implementation(data, shape)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              }
            ],
            "category": "Shape"
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/out_proj/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/Reshape_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__35",
                        "unk__36",
                        "unk__37"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_559",
                  "initializer": {
                    "name": "onnx::MatMul_559",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/out_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__35",
                        "unk__36",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_mha.1/out_proj/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.3.pre_norm_mha.1.out_proj.bias",
                  "initializer": {
                    "name": "transformer.3.pre_norm_mha.1.out_proj.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/out_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__35",
                        "unk__36",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/out_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__35",
                        "unk__36",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.1/out_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__35",
                        "unk__36",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__28",
                        "unk__29",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.3/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/ReduceMean",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Sub",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sub",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary subtraction (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sub",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([3, 2, 1]).astype(np.float32)\nz = x - y  # expected output [-2., 0., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint32\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint64)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint64\")"
              },
              {
                "summary": "sub_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Pow",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Z",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Pow",
            "module": "ai.onnx",
            "version": 13,
            "description": "Pow takes input data (Tensor<T>) and exponent Tensor, and\nproduces one output data (Tensor<T>) where the function `f(x) = x^exponent`,\nis applied to the data tensor elementwise.\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "First operand, base of the exponent."
              },
              {
                "name": "Y",
                "type": "T1",
                "description": "Second operand, power of the exponent."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Z",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input X and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain input Y types to float/int tensors.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "pow",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_example\")\n\nx = np.arange(60).reshape(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow\")"
              },
              {
                "summary": "pow_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array(2).astype(np.float32)\nz = pow(x, y)  # expected output [1., 4., 9.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_scalar\")\n\nnode = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\nx = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\ny = np.array([1, 2, 3]).astype(np.float32)\n# expected output [[1, 4, 27], [4, 25, 216]]\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_array\")"
              },
              {
                "summary": "types",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int32\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint64\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint32\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_int32\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Sqrt",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sqrt",
            "module": "ai.onnx",
            "version": 13,
            "description": "Square root takes one input data (Tensor<T>) and produces one output data\n(Tensor<T>) where the square root is, y = x^0.5, is applied to\nthe tensor elementwise. If x is negative, then it will return NaN.\n",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sqrt",
                "code": "node = onnx.helper.make_node(\n    \"Sqrt\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.array([1, 4, 9]).astype(np.float32)\ny = np.sqrt(x)  # expected output [1., 2., 3.]\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt_example\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\ny = np.sqrt(x)\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.3.pre_norm_ffn.0.weight",
                  "initializer": {
                    "name": "transformer.3.pre_norm_ffn.0.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.3.pre_norm_ffn.0.bias",
                  "initializer": {
                    "name": "transformer.3.pre_norm_ffn.0.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.1/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_560",
                  "initializer": {
                    "name": "onnx::MatMul_560",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          2048
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.1/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.3.pre_norm_ffn.1.bias",
                  "initializer": {
                    "name": "transformer.3.pre_norm_ffn.1.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          2048
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Erf",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Erf_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Erf",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the error function of the given input tensor element-wise.\n",
            "inputs": [
              {
                "name": "input",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "The error function of the input tensor computed element-wise. It has the same shape and type of the input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "erf",
                "code": "node = onnx.helper.make_node(\n    \"Erf\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.random.randn(1, 3, 32, 32).astype(np.float32)\ny = np.vectorize(math.erf)(x).astype(np.float32)\nexpect(node, inputs=[x], outputs=[y], name=\"test_erf\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Erf_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Mul_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Constant_2_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Mul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.4/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.2/Mul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_561",
                  "initializer": {
                    "name": "onnx::MatMul_561",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          2048,
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        2048,
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.4/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.4/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.3.pre_norm_ffn.4.bias",
                  "initializer": {
                    "name": "transformer.3.pre_norm_ffn.4.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.4/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.4/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.3/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.3/pre_norm_ffn/pre_norm_ffn.4/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.3/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.0/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.3/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.0/ReduceMean",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.0/Sub",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sub",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary subtraction (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sub",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([3, 2, 1]).astype(np.float32)\nz = x - y  # expected output [-2., 0., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint32\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint64)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint64\")"
              },
              {
                "summary": "sub_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.0/Pow",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Z",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Pow",
            "module": "ai.onnx",
            "version": 13,
            "description": "Pow takes input data (Tensor<T>) and exponent Tensor, and\nproduces one output data (Tensor<T>) where the function `f(x) = x^exponent`,\nis applied to the data tensor elementwise.\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "First operand, base of the exponent."
              },
              {
                "name": "Y",
                "type": "T1",
                "description": "Second operand, power of the exponent."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Z",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input X and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain input Y types to float/int tensors.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "pow",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_example\")\n\nx = np.arange(60).reshape(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow\")"
              },
              {
                "summary": "pow_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array(2).astype(np.float32)\nz = pow(x, y)  # expected output [1., 4., 9.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_scalar\")\n\nnode = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\nx = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\ny = np.array([1, 2, 3]).astype(np.float32)\n# expected output [[1, 4, 27], [4, 25, 216]]\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_array\")"
              },
              {
                "summary": "types",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int32\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint64\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint32\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_int32\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.0/ReduceMean_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.0/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.0/Sqrt",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sqrt",
            "module": "ai.onnx",
            "version": 13,
            "description": "Square root takes one input data (Tensor<T>) and produces one output data\n(Tensor<T>) where the square root is, y = x^0.5, is applied to\nthe tensor elementwise. If x is negative, then it will return NaN.\n",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sqrt",
                "code": "node = onnx.helper.make_node(\n    \"Sqrt\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.array([1, 4, 9]).astype(np.float32)\ny = np.sqrt(x)  # expected output [1., 2., 3.]\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt_example\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\ny = np.sqrt(x)\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.0/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.0/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.4.pre_norm_mha.0.weight",
                  "initializer": {
                    "name": "transformer.4.pre_norm_mha.0.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.0/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.4.pre_norm_mha.0.bias",
                  "initializer": {
                    "name": "transformer.4.pre_norm_mha.0.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.0/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Shape",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Shape_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Shape",
            "module": "ai.onnx",
            "version": 13,
            "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "shape",
                "type": "T1",
                "description": "Shape of the input tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Input tensor can be of arbitrary type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain output to int64 tensor.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "shape",
                "code": "x = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n).astype(np.float32)\ntest_shape(\"_example\", x)  # preserve names of original test cases\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\n\ntest_shape(\"\", x)  # preserve names of original test cases\n\ntest_shape(\"_start_1\", x, start=1)\n\ntest_shape(\"_end_1\", x, end=1)\n\ntest_shape(\"_start_negative_1\", x, start=-1)\n\ntest_shape(\"_end_negative_1\", x, end=-1)\n\ntest_shape(\"_start_1_end_negative_1\", x, start=1, end=-1)\n\ntest_shape(\"_start_1_end_2\", x, start=1, end=2)\n\ntest_shape(\"_clip_start\", x, start=-10)\n\ntest_shape(\"_clip_end\", x, end=10)"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Gather",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Shape_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Shape_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Shape_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Shape",
            "module": "ai.onnx",
            "version": 13,
            "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "shape",
                "type": "T1",
                "description": "Shape of the input tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Input tensor can be of arbitrary type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain output to int64 tensor.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "shape",
                "code": "x = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n).astype(np.float32)\ntest_shape(\"_example\", x)  # preserve names of original test cases\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\n\ntest_shape(\"\", x)  # preserve names of original test cases\n\ntest_shape(\"_start_1\", x, start=1)\n\ntest_shape(\"_end_1\", x, end=1)\n\ntest_shape(\"_start_negative_1\", x, start=-1)\n\ntest_shape(\"_end_negative_1\", x, end=-1)\n\ntest_shape(\"_start_1_end_negative_1\", x, start=1, end=-1)\n\ntest_shape(\"_start_1_end_2\", x, start=1, end=2)\n\ntest_shape(\"_clip_start\", x, start=-10)\n\ntest_shape(\"_clip_end\", x, end=10)"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Gather_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Shape_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/qkv_proj/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_562",
                  "initializer": {
                    "name": "onnx::MatMul_562",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          1536
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/qkv_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/qkv_proj/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.4.pre_norm_mha.1.qkv_proj.bias",
                  "initializer": {
                    "name": "transformer.4.pre_norm_mha.1.qkv_proj.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          1536
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        1536
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/qkv_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/qkv_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1536
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Unsqueeze",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_406",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Unsqueeze_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Unsqueeze_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_408",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Unsqueeze_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Concat",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "inputs",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Unsqueeze_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.4/pre_norm_mha.1/Unsqueeze_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.4/pre_norm_mha.1/Constant_2_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 3,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.4/pre_norm_mha.1/Constant_3_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 8,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.4/pre_norm_mha.1/Constant_4_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 255,
                      "1": 255,
                      "2": 255,
                      "3": 255,
                      "4": 255,
                      "5": 255,
                      "6": 255,
                      "7": 255
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "concat_result",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Concat_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        5
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Concat",
            "module": "ai.onnx",
            "version": 13,
            "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": true,
                "description": "Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs).."
              }
            ],
            "inputs": [
              {
                "name": "inputs",
                "type": "T",
                "list": true,
                "description": "List of tensors for concatenation"
              }
            ],
            "min_input": 1,
            "max_input": 2147483647,
            "outputs": [
              {
                "name": "concat_result",
                "type": "T",
                "description": "Concatenated tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - &#8734;",
            "type_constraints": [
              {
                "description": "Constrain output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "concat",
                "code": "test_cases: dict[str, Sequence[Any]] = {\n    \"1d\": ([1, 2], [3, 4]),\n    \"2d\": ([[1, 2], [3, 4]], [[5, 6], [7, 8]]),\n    \"3d\": (\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        [[[9, 10], [11, 12]], [[13, 14], [15, 16]]],\n    ),\n}\n\nfor test_case, values_ in test_cases.items():\n    values = [np.asarray(v, dtype=np.float32) for v in values_]\n    for i in range(len(values[0].shape)):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_\" + str(i),\n        )\n\n    for i in range(-len(values[0].shape), 0):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_negative_\" + str(abs(i)),\n        )"
              }
            ],
            "category": "Tensor"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Reshape",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/qkv_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        1536
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Concat_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        5
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reshaped",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Reshape_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__41",
                        "unk__42",
                        "unk__43",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "allowzero",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Reshape",
            "module": "ai.onnx",
            "version": 14,
            "description": "Reshape the input tensor similar to numpy.reshape.\nFirst input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\nAt most one dimension of the new shape can be -1. In this case, the value is\ninferred from the size of the tensor and the remaining dimensions. A dimension\ncould also be 0, in which case the actual dimension value is unchanged (i.e. taken\nfrom the input tensor). If 'allowzero' is set, and the new shape includes 0, the\ndimension will be set explicitly to zero (i.e. not taken from input tensor).\nShape (second input) could be an empty shape, which means converting to a scalar.\nThe input tensor's shape and the output tensor's shape are required to have the same number of elements.\n\nIf the attribute 'allowzero' is set, it is invalid for the specified shape to\ncontain both a zero value and -1, as the value of the dimension corresponding\nto -1 cannot be determined uniquely.\n",
            "attributes": [
              {
                "name": "allowzero",
                "type": "int64",
                "required": false,
                "description": "(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              },
              {
                "name": "shape",
                "type": "tensor(int64)",
                "description": "Specified shape for output."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "reshaped",
                "type": "T",
                "description": "Reshaped data."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "allowzero",
                "code": "original_shape = [0, 3, 4]\ntest_cases = {\n    \"allowzero_reordered\": np.array([3, 4, 0], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n        allowzero=1,  # if allowzero=1, final shape = (3, 4, 0)\n        # if allowzero=0, final shape = (3, 4, 4)\n    )\n\n    reshaped = reshape_reference_implementation(data, shape, allowzero=1)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              },
              {
                "summary": "reshape",
                "code": "original_shape = [2, 3, 4]\ntest_cases = {\n    \"reordered_all_dims\": np.array([4, 2, 3], dtype=np.int64),\n    \"reordered_last_dims\": np.array([2, 4, 3], dtype=np.int64),\n    \"reduced_dims\": np.array([2, 12], dtype=np.int64),\n    \"extended_dims\": np.array([2, 3, 2, 2], dtype=np.int64),\n    \"one_dim\": np.array([24], dtype=np.int64),\n    \"negative_dim\": np.array([2, -1, 2], dtype=np.int64),\n    \"negative_extended_dims\": np.array([-1, 2, 3, 4], dtype=np.int64),\n    \"zero_dim\": np.array([2, 0, 4, 1], dtype=np.int64),\n    \"zero_and_negative_dim\": np.array([2, 0, 1, -1], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n    )\n\n    reshaped = reshape_reference_implementation(data, shape)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              }
            ],
            "category": "Shape"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Transpose",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Reshape_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__41",
                        "unk__42",
                        "unk__43",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__42",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "3",
                  "2",
                  "1",
                  "4"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Gather_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__42",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.1/pre_norm_mha.1/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Gather_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Gather_3",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__42",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Gather_3_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Gather_4",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__42",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/transformer.0/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Gather_4_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Gather_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Constant_5_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Transpose_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Gather_3_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Transpose_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__44",
                        "unk__41"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "1",
                  "3",
                  "2"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Transpose_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__44",
                        "unk__41"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__41"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__41"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__41"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/softmax/Softmax",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__41"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/softmax/Softmax_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__41"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "-1"
              }
            }
          ],
          "type": {
            "name": "Softmax",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator computes the normalized exponential values for the given input:\n\n Softmax(input, axis) = Exp(input) / ReduceSum(Exp(input), axis=axis, keepdims=1) \n\nThe \"axis\" attribute indicates the dimension along which Softmax\nwill be performed. The output tensor has the same shape\nand contains the Softmax values of the corresponding input.\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "default": -1,
                "description": "\nDescribes the dimension Softmax will be performed on.\nNegative value means counting dimensions\nfrom the back. Accepted range is [-r, r-1] where r = rank(input).\n"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T",
                "description": "The input tensor of rank >= axis."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "The output values with the same shape as the input tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "softmax",
                "code": "node = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\nx = np.array([[-1, 0, 1]]).astype(np.float32)\n# expected output [[0.09003058, 0.24472848, 0.66524094]]\ny = softmax(x, axis=1)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_example\")"
              },
              {
                "summary": "softmax_axis",
                "code": "x = np.array([[0, 1, 2, 3], [10000, 10001, 10002, 10003]]).astype(np.float32)\n# expected output\n# [[0.032058604 0.08714432  0.23688284  0.6439143  ]\n# [0.032058604 0.08714432  0.23688284  0.6439143  ]]\ny = softmax(x)\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_large_number\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ny = softmax(x, axis=0)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_axis_0\")\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ny = softmax(x, axis=1)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_axis_1\")\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=2,\n)\ny = softmax(x, axis=2)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_axis_2\")\n\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n    axis=-1,\n)\ny = softmax(x, axis=-1)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_negative_axis\")\n\n# default axis is -1\nnode = onnx.helper.make_node(\n    \"Softmax\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\nexpect(node, inputs=[x], outputs=[y], name=\"test_softmax_default_axis\")"
              }
            ],
            "category": "Activation"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/softmax/Softmax_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__41"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__41"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/MatMul_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__41"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Gather_4_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/MatMul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Transpose_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/MatMul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__43",
                        "unk__41",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Transpose_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__41",
                        "unk__43",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "2",
                  "1",
                  "3"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Unsqueeze_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Gather_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_431",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Unsqueeze_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Unsqueeze_3",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "onnx::Unsqueeze_433",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 0,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Unsqueeze_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Concat_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "inputs",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Unsqueeze_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.4/pre_norm_mha.1/Unsqueeze_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/transformer.4/pre_norm_mha.1/Constant_6_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 255,
                      "1": 255,
                      "2": 255,
                      "3": 255,
                      "4": 255,
                      "5": 255,
                      "6": 255,
                      "7": 255
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "concat_result",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Concat_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Concat",
            "module": "ai.onnx",
            "version": 13,
            "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": true,
                "description": "Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs).."
              }
            ],
            "inputs": [
              {
                "name": "inputs",
                "type": "T",
                "list": true,
                "description": "List of tensors for concatenation"
              }
            ],
            "min_input": 1,
            "max_input": 2147483647,
            "outputs": [
              {
                "name": "concat_result",
                "type": "T",
                "description": "Concatenated tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - &#8734;",
            "type_constraints": [
              {
                "description": "Constrain output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "concat",
                "code": "test_cases: dict[str, Sequence[Any]] = {\n    \"1d\": ([1, 2], [3, 4]),\n    \"2d\": ([[1, 2], [3, 4]], [[5, 6], [7, 8]]),\n    \"3d\": (\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        [[[9, 10], [11, 12]], [[13, 14], [15, 16]]],\n    ),\n}\n\nfor test_case, values_ in test_cases.items():\n    values = [np.asarray(v, dtype=np.float32) for v in values_]\n    for i in range(len(values[0].shape)):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_\" + str(i),\n        )\n\n    for i in range(-len(values[0].shape), 0):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_negative_\" + str(abs(i)),\n        )"
              }
            ],
            "category": "Tensor"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/Reshape_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Transpose_2_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__40",
                        "unk__41",
                        "unk__43",
                        "unk__44"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "shape",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Concat_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reshaped",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Reshape_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__45",
                        "unk__46",
                        "unk__47"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "allowzero",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Reshape",
            "module": "ai.onnx",
            "version": 14,
            "description": "Reshape the input tensor similar to numpy.reshape.\nFirst input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\nAt most one dimension of the new shape can be -1. In this case, the value is\ninferred from the size of the tensor and the remaining dimensions. A dimension\ncould also be 0, in which case the actual dimension value is unchanged (i.e. taken\nfrom the input tensor). If 'allowzero' is set, and the new shape includes 0, the\ndimension will be set explicitly to zero (i.e. not taken from input tensor).\nShape (second input) could be an empty shape, which means converting to a scalar.\nThe input tensor's shape and the output tensor's shape are required to have the same number of elements.\n\nIf the attribute 'allowzero' is set, it is invalid for the specified shape to\ncontain both a zero value and -1, as the value of the dimension corresponding\nto -1 cannot be determined uniquely.\n",
            "attributes": [
              {
                "name": "allowzero",
                "type": "int64",
                "required": false,
                "description": "(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              },
              {
                "name": "shape",
                "type": "tensor(int64)",
                "description": "Specified shape for output."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "reshaped",
                "type": "T",
                "description": "Reshaped data."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "allowzero",
                "code": "original_shape = [0, 3, 4]\ntest_cases = {\n    \"allowzero_reordered\": np.array([3, 4, 0], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n        allowzero=1,  # if allowzero=1, final shape = (3, 4, 0)\n        # if allowzero=0, final shape = (3, 4, 4)\n    )\n\n    reshaped = reshape_reference_implementation(data, shape, allowzero=1)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              },
              {
                "summary": "reshape",
                "code": "original_shape = [2, 3, 4]\ntest_cases = {\n    \"reordered_all_dims\": np.array([4, 2, 3], dtype=np.int64),\n    \"reordered_last_dims\": np.array([2, 4, 3], dtype=np.int64),\n    \"reduced_dims\": np.array([2, 12], dtype=np.int64),\n    \"extended_dims\": np.array([2, 3, 2, 2], dtype=np.int64),\n    \"one_dim\": np.array([24], dtype=np.int64),\n    \"negative_dim\": np.array([2, -1, 2], dtype=np.int64),\n    \"negative_extended_dims\": np.array([-1, 2, 3, 4], dtype=np.int64),\n    \"zero_dim\": np.array([2, 0, 4, 1], dtype=np.int64),\n    \"zero_and_negative_dim\": np.array([2, 0, 1, -1], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n    )\n\n    reshaped = reshape_reference_implementation(data, shape)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              }
            ],
            "category": "Shape"
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/out_proj/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/Reshape_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__45",
                        "unk__46",
                        "unk__47"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_567",
                  "initializer": {
                    "name": "onnx::MatMul_567",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/out_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__45",
                        "unk__46",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_mha.1/out_proj/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.4.pre_norm_mha.1.out_proj.bias",
                  "initializer": {
                    "name": "transformer.4.pre_norm_mha.1.out_proj.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/out_proj/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__45",
                        "unk__46",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/out_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__45",
                        "unk__46",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.1/out_proj/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__45",
                        "unk__46",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_mha.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__38",
                        "unk__39",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.4/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/ReduceMean",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Sub",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sub",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary subtraction (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sub",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([3, 2, 1]).astype(np.float32)\nz = x - y  # expected output [-2., 0., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint32\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint64)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint64\")"
              },
              {
                "summary": "sub_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Pow",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Z",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Pow",
            "module": "ai.onnx",
            "version": 13,
            "description": "Pow takes input data (Tensor<T>) and exponent Tensor, and\nproduces one output data (Tensor<T>) where the function `f(x) = x^exponent`,\nis applied to the data tensor elementwise.\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "First operand, base of the exponent."
              },
              {
                "name": "Y",
                "type": "T1",
                "description": "Second operand, power of the exponent."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Z",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input X and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain input Y types to float/int tensors.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "pow",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_example\")\n\nx = np.arange(60).reshape(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow\")"
              },
              {
                "summary": "pow_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array(2).astype(np.float32)\nz = pow(x, y)  # expected output [1., 4., 9.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_scalar\")\n\nnode = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\nx = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\ny = np.array([1, 2, 3]).astype(np.float32)\n# expected output [[1, 4, 27], [4, 25, 216]]\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_array\")"
              },
              {
                "summary": "types",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int32\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint64\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint32\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_int32\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Sqrt",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sqrt",
            "module": "ai.onnx",
            "version": 13,
            "description": "Square root takes one input data (Tensor<T>) and produces one output data\n(Tensor<T>) where the square root is, y = x^0.5, is applied to\nthe tensor elementwise. If x is negative, then it will return NaN.\n",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sqrt",
                "code": "node = onnx.helper.make_node(\n    \"Sqrt\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.array([1, 4, 9]).astype(np.float32)\ny = np.sqrt(x)  # expected output [1., 2., 3.]\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt_example\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\ny = np.sqrt(x)\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.4.pre_norm_ffn.0.weight",
                  "initializer": {
                    "name": "transformer.4.pre_norm_ffn.0.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.4.pre_norm_ffn.0.bias",
                  "initializer": {
                    "name": "transformer.4.pre_norm_ffn.0.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.1/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_568",
                  "initializer": {
                    "name": "onnx::MatMul_568",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          2048
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.1/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.4.pre_norm_ffn.1.bias",
                  "initializer": {
                    "name": "transformer.4.pre_norm_ffn.1.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          2048
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.1/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Erf",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Erf_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Erf",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the error function of the given input tensor element-wise.\n",
            "inputs": [
              {
                "name": "input",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "The error function of the input tensor computed element-wise. It has the same shape and type of the input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "erf",
                "code": "node = onnx.helper.make_node(\n    \"Erf\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.random.randn(1, 3, 32, 32).astype(np.float32)\ny = np.vectorize(math.erf)(x).astype(np.float32)\nexpect(node, inputs=[x], outputs=[y], name=\"test_erf\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Erf_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.1/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Mul_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Constant_2_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Mul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.4/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.2/Mul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        2048
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::MatMul_569",
                  "initializer": {
                    "name": "onnx::MatMul_569",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          2048,
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        2048,
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.4/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.4/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.4.pre_norm_ffn.4.bias",
                  "initializer": {
                    "name": "transformer.4.pre_norm_ffn.4.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.4/MatMul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.4/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.4/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.0/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.4/pre_norm_ffn/pre_norm_ffn.4/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.4/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.5/Transpose",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.4/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__49",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.5/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        "unk__49"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "2",
                  "1"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.5/Unsqueeze",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.5/Transpose_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        "unk__49"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "/transformer.5/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 2,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "expanded",
              "value": [
                {
                  "name": "/transformer.5/Unsqueeze_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__49"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Unsqueeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\nTakes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n\nFor example, given an input tensor (`data`) of shape [3, 4, 5], then\nUnsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\nThe rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\nEach value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values in `axes` does not matter and can come in any order.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Original tensor"
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "description": "List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded)."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "expanded",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "unsqueeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2]).astype(np.int64)\ny = np.expand_dims(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_negative_axes\")"
              },
              {
                "summary": "unsqueeze_one_axis",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\n\nfor i in range(x.ndim):\n    axes = np.array([i]).astype(np.int64)\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        inputs=[\"x\", \"axes\"],\n        outputs=[\"y\"],\n    )\n    y = np.expand_dims(x, axis=i)\n\n    expect(\n        node,\n        inputs=[x, axes],\n        outputs=[y],\n        name=\"test_unsqueeze_axis_\" + str(i),\n    )"
              },
              {
                "summary": "unsqueeze_three_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([2, 4, 5]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_three_axes\")"
              },
              {
                "summary": "unsqueeze_two_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([1, 4]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=1)\ny = np.expand_dims(y, axis=4)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_two_axes\")"
              },
              {
                "summary": "unsqueeze_unsorted_axes",
                "code": "x = np.random.randn(3, 4, 5).astype(np.float32)\naxes = np.array([5, 4, 2]).astype(np.int64)\n\nnode = onnx.helper.make_node(\n    \"Unsqueeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\ny = np.expand_dims(x, axis=2)\ny = np.expand_dims(y, axis=4)\ny = np.expand_dims(y, axis=5)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_unsqueeze_unsorted_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.5/token_mixer/reparam_conv/Conv",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.5/Unsqueeze_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__49"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "W",
              "value": [
                {
                  "name": "transformer.5.token_mixer.reparam_conv.weight",
                  "initializer": {
                    "name": "transformer.5.token_mixer.reparam_conv.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          1,
                          1,
                          11
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        1,
                        1,
                        11
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.5.token_mixer.reparam_conv.bias",
                  "initializer": {
                    "name": "transformer.5.token_mixer.reparam_conv.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.5/token_mixer/reparam_conv/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__50"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "dilations",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            },
            {
              "name": "group",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "512"
              }
            },
            {
              "name": "kernel_shape",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "11"
                ]
              }
            },
            {
              "name": "pads",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "5",
                  "0",
                  "5"
                ]
              }
            },
            {
              "name": "strides",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            }
          ],
          "type": {
            "name": "Conv",
            "module": "ai.onnx",
            "version": 11,
            "description": "The convolution operator consumes an input tensor and a filter, and\ncomputes the output.",
            "attributes": [
              {
                "name": "auto_pad",
                "type": "string",
                "required": false,
                "default": "NOTSET",
                "description": "auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER."
              },
              {
                "name": "dilations",
                "type": "int64[]",
                "required": false,
                "description": "dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis."
              },
              {
                "name": "group",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "number of groups input channels and output channels are divided into."
              },
              {
                "name": "kernel_shape",
                "type": "int64[]",
                "required": false,
                "description": "The shape of the convolution kernel. If not present, should be inferred from input W."
              },
              {
                "name": "pads",
                "type": "int64[]",
                "required": false,
                "description": "Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis."
              },
              {
                "name": "strides",
                "type": "int64[]",
                "required": false,
                "description": "Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis."
              }
            ],
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input data tensor from previous layer; has size (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and width. Note that this is for the 2D image. Otherwise the size is (N x C x D1 x D2 ... x Dn). Optionally, if dimension denotation is in effect, the operation expects input data tensor to arrive with the dimension denotation of [DATA_BATCH, DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE ...]."
              },
              {
                "name": "W",
                "type": "T",
                "description": "The weight tensor that will be used in the convolutions; has size (M x C/group x kH x kW), where C is the number of channels, and kH and kW are the height and width of the kernel, and M is the number of feature maps. For more than 2 dimensions, the kernel shape will be (M x C/group x k1 x k2 x ... x kn), where (k1 x k2 x ... kn) is the dimension of the kernel. Optionally, if dimension denotation is in effect, the operation expects the weight tensor to arrive with the dimension denotation of [FILTER_OUT_CHANNEL, FILTER_IN_CHANNEL, FILTER_SPATIAL, FILTER_SPATIAL ...]. Assuming zero based indices for the shape array, X.shape[1] == (W.shape[1] * group) == C and W.shape[0] mod G == 0. Or in other words FILTER_IN_CHANNEL multiplied by the number of groups should be equal to DATA_CHANNEL and the number of feature maps M should be a multiple of the number of groups G."
              },
              {
                "name": "B",
                "type": "T",
                "option": "optional",
                "description": "Optional 1D bias to be added to the convolution, has size of M."
              }
            ],
            "min_input": 2,
            "max_input": 3,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output data tensor that contains the result of the convolution. The output dimensions are functions of the kernel size, stride size, and pad lengths."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "2 - 3",
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "conv",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[1, 1, 1, 1],\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 21.0, 27.0, 33.0, 24.0],  # (1, 1, 5, 5) output tensor\n                [33.0, 54.0, 63.0, 72.0, 51.0],\n                [63.0, 99.0, 108.0, 117.0, 81.0],\n                [93.0, 144.0, 153.0, 162.0, 111.0],\n                [72.0, 111.0, 117.0, 123.0, 84.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_basic_conv_with_padding\",\n)\n\n# Convolution without padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[0, 0, 0, 0],\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 63.0, 72.0],  # (1, 1, 3, 3) output tensor\n                [99.0, 108.0, 117.0],\n                [144.0, 153.0, 162.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_basic_conv_without_padding\",\n)"
              },
              {
                "summary": "conv_with_autopad_same",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with auto_pad='SAME_LOWER' and strides=2\nnode = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    auto_pad=\"SAME_LOWER\",\n    kernel_shape=[3, 3],\n    strides=[2, 2],\n)\ny = np.array(\n    [[[[12.0, 27.0, 24.0], [63.0, 108.0, 81.0], [72.0, 117.0, 84.0]]]]\n).astype(np.float32)\nexpect(node, inputs=[x, W], outputs=[y], name=\"test_conv_with_autopad_same\")"
              },
              {
                "summary": "conv_with_strides",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 7, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n                [25.0, 26.0, 27.0, 28.0, 29.0],\n                [30.0, 31.0, 32.0, 33.0, 34.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with strides=2 and padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 1, 1, 1],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 27.0, 24.0],  # (1, 1, 4, 3) output tensor\n                [63.0, 108.0, 81.0],\n                [123.0, 198.0, 141.0],\n                [112.0, 177.0, 124.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_conv_with_strides_padding\",\n)\n\n# Convolution with strides=2 and no padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[0, 0, 0, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 72.0],  # (1, 1, 3, 2) output tensor\n                [144.0, 162.0],\n                [234.0, 252.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_conv_with_strides_no_padding\",\n)\n\n# Convolution with strides=2 and padding only along one dimension (the H dimension in NxCxHxW tensor)\nnode_with_asymmetric_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 0, 1, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_asymmetric_padding = np.array(\n    [\n        [\n            [\n                [21.0, 33.0],  # (1, 1, 4, 2) output tensor\n                [99.0, 117.0],\n                [189.0, 207.0],\n                [171.0, 183.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_asymmetric_padding,\n    inputs=[x, W],\n    outputs=[y_with_asymmetric_padding],\n    name=\"test_conv_with_strides_and_asymmetric_padding\",\n)"
              }
            ],
            "category": "Layer"
          }
        },
        {
          "name": "/transformer.5/convffn/conv/conv/Conv",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.5/token_mixer/reparam_conv/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__50"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "W",
              "value": [
                {
                  "name": "onnx::Conv_534",
                  "initializer": {
                    "name": "onnx::Conv_534",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          1,
                          1,
                          11
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        1,
                        1,
                        11
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "onnx::Conv_535",
                  "initializer": {
                    "name": "onnx::Conv_535",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.5/convffn/conv/conv/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__51"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "dilations",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            },
            {
              "name": "group",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "512"
              }
            },
            {
              "name": "kernel_shape",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "11"
                ]
              }
            },
            {
              "name": "pads",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "5",
                  "0",
                  "5"
                ]
              }
            },
            {
              "name": "strides",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            }
          ],
          "type": {
            "name": "Conv",
            "module": "ai.onnx",
            "version": 11,
            "description": "The convolution operator consumes an input tensor and a filter, and\ncomputes the output.",
            "attributes": [
              {
                "name": "auto_pad",
                "type": "string",
                "required": false,
                "default": "NOTSET",
                "description": "auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER."
              },
              {
                "name": "dilations",
                "type": "int64[]",
                "required": false,
                "description": "dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis."
              },
              {
                "name": "group",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "number of groups input channels and output channels are divided into."
              },
              {
                "name": "kernel_shape",
                "type": "int64[]",
                "required": false,
                "description": "The shape of the convolution kernel. If not present, should be inferred from input W."
              },
              {
                "name": "pads",
                "type": "int64[]",
                "required": false,
                "description": "Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis."
              },
              {
                "name": "strides",
                "type": "int64[]",
                "required": false,
                "description": "Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis."
              }
            ],
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input data tensor from previous layer; has size (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and width. Note that this is for the 2D image. Otherwise the size is (N x C x D1 x D2 ... x Dn). Optionally, if dimension denotation is in effect, the operation expects input data tensor to arrive with the dimension denotation of [DATA_BATCH, DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE ...]."
              },
              {
                "name": "W",
                "type": "T",
                "description": "The weight tensor that will be used in the convolutions; has size (M x C/group x kH x kW), where C is the number of channels, and kH and kW are the height and width of the kernel, and M is the number of feature maps. For more than 2 dimensions, the kernel shape will be (M x C/group x k1 x k2 x ... x kn), where (k1 x k2 x ... kn) is the dimension of the kernel. Optionally, if dimension denotation is in effect, the operation expects the weight tensor to arrive with the dimension denotation of [FILTER_OUT_CHANNEL, FILTER_IN_CHANNEL, FILTER_SPATIAL, FILTER_SPATIAL ...]. Assuming zero based indices for the shape array, X.shape[1] == (W.shape[1] * group) == C and W.shape[0] mod G == 0. Or in other words FILTER_IN_CHANNEL multiplied by the number of groups should be equal to DATA_CHANNEL and the number of feature maps M should be a multiple of the number of groups G."
              },
              {
                "name": "B",
                "type": "T",
                "option": "optional",
                "description": "Optional 1D bias to be added to the convolution, has size of M."
              }
            ],
            "min_input": 2,
            "max_input": 3,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output data tensor that contains the result of the convolution. The output dimensions are functions of the kernel size, stride size, and pad lengths."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "2 - 3",
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "conv",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[1, 1, 1, 1],\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 21.0, 27.0, 33.0, 24.0],  # (1, 1, 5, 5) output tensor\n                [33.0, 54.0, 63.0, 72.0, 51.0],\n                [63.0, 99.0, 108.0, 117.0, 81.0],\n                [93.0, 144.0, 153.0, 162.0, 111.0],\n                [72.0, 111.0, 117.0, 123.0, 84.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_basic_conv_with_padding\",\n)\n\n# Convolution without padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[0, 0, 0, 0],\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 63.0, 72.0],  # (1, 1, 3, 3) output tensor\n                [99.0, 108.0, 117.0],\n                [144.0, 153.0, 162.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_basic_conv_without_padding\",\n)"
              },
              {
                "summary": "conv_with_autopad_same",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with auto_pad='SAME_LOWER' and strides=2\nnode = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    auto_pad=\"SAME_LOWER\",\n    kernel_shape=[3, 3],\n    strides=[2, 2],\n)\ny = np.array(\n    [[[[12.0, 27.0, 24.0], [63.0, 108.0, 81.0], [72.0, 117.0, 84.0]]]]\n).astype(np.float32)\nexpect(node, inputs=[x, W], outputs=[y], name=\"test_conv_with_autopad_same\")"
              },
              {
                "summary": "conv_with_strides",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 7, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n                [25.0, 26.0, 27.0, 28.0, 29.0],\n                [30.0, 31.0, 32.0, 33.0, 34.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with strides=2 and padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 1, 1, 1],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 27.0, 24.0],  # (1, 1, 4, 3) output tensor\n                [63.0, 108.0, 81.0],\n                [123.0, 198.0, 141.0],\n                [112.0, 177.0, 124.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_conv_with_strides_padding\",\n)\n\n# Convolution with strides=2 and no padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[0, 0, 0, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 72.0],  # (1, 1, 3, 2) output tensor\n                [144.0, 162.0],\n                [234.0, 252.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_conv_with_strides_no_padding\",\n)\n\n# Convolution with strides=2 and padding only along one dimension (the H dimension in NxCxHxW tensor)\nnode_with_asymmetric_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 0, 1, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_asymmetric_padding = np.array(\n    [\n        [\n            [\n                [21.0, 33.0],  # (1, 1, 4, 2) output tensor\n                [99.0, 117.0],\n                [189.0, 207.0],\n                [171.0, 183.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_asymmetric_padding,\n    inputs=[x, W],\n    outputs=[y_with_asymmetric_padding],\n    name=\"test_conv_with_strides_and_asymmetric_padding\",\n)"
              }
            ],
            "category": "Layer"
          }
        },
        {
          "name": "/transformer.5/convffn/fc1/Conv",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.5/convffn/conv/conv/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__51"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "W",
              "value": [
                {
                  "name": "transformer.5.convffn.fc1.weight",
                  "initializer": {
                    "name": "transformer.5.convffn.fc1.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          2048,
                          512,
                          1,
                          1
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        2048,
                        512,
                        1,
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.5.convffn.fc1.bias",
                  "initializer": {
                    "name": "transformer.5.convffn.fc1.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          2048
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        2048
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.5/convffn/fc1/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "dilations",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            },
            {
              "name": "group",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "1"
              }
            },
            {
              "name": "kernel_shape",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            },
            {
              "name": "pads",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "0",
                  "0",
                  "0"
                ]
              }
            },
            {
              "name": "strides",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            }
          ],
          "type": {
            "name": "Conv",
            "module": "ai.onnx",
            "version": 11,
            "description": "The convolution operator consumes an input tensor and a filter, and\ncomputes the output.",
            "attributes": [
              {
                "name": "auto_pad",
                "type": "string",
                "required": false,
                "default": "NOTSET",
                "description": "auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER."
              },
              {
                "name": "dilations",
                "type": "int64[]",
                "required": false,
                "description": "dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis."
              },
              {
                "name": "group",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "number of groups input channels and output channels are divided into."
              },
              {
                "name": "kernel_shape",
                "type": "int64[]",
                "required": false,
                "description": "The shape of the convolution kernel. If not present, should be inferred from input W."
              },
              {
                "name": "pads",
                "type": "int64[]",
                "required": false,
                "description": "Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis."
              },
              {
                "name": "strides",
                "type": "int64[]",
                "required": false,
                "description": "Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis."
              }
            ],
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input data tensor from previous layer; has size (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and width. Note that this is for the 2D image. Otherwise the size is (N x C x D1 x D2 ... x Dn). Optionally, if dimension denotation is in effect, the operation expects input data tensor to arrive with the dimension denotation of [DATA_BATCH, DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE ...]."
              },
              {
                "name": "W",
                "type": "T",
                "description": "The weight tensor that will be used in the convolutions; has size (M x C/group x kH x kW), where C is the number of channels, and kH and kW are the height and width of the kernel, and M is the number of feature maps. For more than 2 dimensions, the kernel shape will be (M x C/group x k1 x k2 x ... x kn), where (k1 x k2 x ... kn) is the dimension of the kernel. Optionally, if dimension denotation is in effect, the operation expects the weight tensor to arrive with the dimension denotation of [FILTER_OUT_CHANNEL, FILTER_IN_CHANNEL, FILTER_SPATIAL, FILTER_SPATIAL ...]. Assuming zero based indices for the shape array, X.shape[1] == (W.shape[1] * group) == C and W.shape[0] mod G == 0. Or in other words FILTER_IN_CHANNEL multiplied by the number of groups should be equal to DATA_CHANNEL and the number of feature maps M should be a multiple of the number of groups G."
              },
              {
                "name": "B",
                "type": "T",
                "option": "optional",
                "description": "Optional 1D bias to be added to the convolution, has size of M."
              }
            ],
            "min_input": 2,
            "max_input": 3,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output data tensor that contains the result of the convolution. The output dimensions are functions of the kernel size, stride size, and pad lengths."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "2 - 3",
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "conv",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[1, 1, 1, 1],\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 21.0, 27.0, 33.0, 24.0],  # (1, 1, 5, 5) output tensor\n                [33.0, 54.0, 63.0, 72.0, 51.0],\n                [63.0, 99.0, 108.0, 117.0, 81.0],\n                [93.0, 144.0, 153.0, 162.0, 111.0],\n                [72.0, 111.0, 117.0, 123.0, 84.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_basic_conv_with_padding\",\n)\n\n# Convolution without padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[0, 0, 0, 0],\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 63.0, 72.0],  # (1, 1, 3, 3) output tensor\n                [99.0, 108.0, 117.0],\n                [144.0, 153.0, 162.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_basic_conv_without_padding\",\n)"
              },
              {
                "summary": "conv_with_autopad_same",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with auto_pad='SAME_LOWER' and strides=2\nnode = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    auto_pad=\"SAME_LOWER\",\n    kernel_shape=[3, 3],\n    strides=[2, 2],\n)\ny = np.array(\n    [[[[12.0, 27.0, 24.0], [63.0, 108.0, 81.0], [72.0, 117.0, 84.0]]]]\n).astype(np.float32)\nexpect(node, inputs=[x, W], outputs=[y], name=\"test_conv_with_autopad_same\")"
              },
              {
                "summary": "conv_with_strides",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 7, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n                [25.0, 26.0, 27.0, 28.0, 29.0],\n                [30.0, 31.0, 32.0, 33.0, 34.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with strides=2 and padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 1, 1, 1],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 27.0, 24.0],  # (1, 1, 4, 3) output tensor\n                [63.0, 108.0, 81.0],\n                [123.0, 198.0, 141.0],\n                [112.0, 177.0, 124.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_conv_with_strides_padding\",\n)\n\n# Convolution with strides=2 and no padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[0, 0, 0, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 72.0],  # (1, 1, 3, 2) output tensor\n                [144.0, 162.0],\n                [234.0, 252.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_conv_with_strides_no_padding\",\n)\n\n# Convolution with strides=2 and padding only along one dimension (the H dimension in NxCxHxW tensor)\nnode_with_asymmetric_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 0, 1, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_asymmetric_padding = np.array(\n    [\n        [\n            [\n                [21.0, 33.0],  # (1, 1, 4, 2) output tensor\n                [99.0, 117.0],\n                [189.0, 207.0],\n                [171.0, 183.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_asymmetric_padding,\n    inputs=[x, W],\n    outputs=[y_with_asymmetric_padding],\n    name=\"test_conv_with_strides_and_asymmetric_padding\",\n)"
              }
            ],
            "category": "Layer"
          }
        },
        {
          "name": "/transformer.5/convffn/act/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.5/convffn/fc1/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.5/convffn/act/Erf",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Erf_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Erf",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the error function of the given input tensor element-wise.\n",
            "inputs": [
              {
                "name": "input",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "The error function of the input tensor computed element-wise. It has the same shape and type of the input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "erf",
                "code": "node = onnx.helper.make_node(\n    \"Erf\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.random.randn(1, 3, 32, 32).astype(np.float32)\ny = np.vectorize(math.erf)(x).astype(np.float32)\nexpect(node, inputs=[x], outputs=[y], name=\"test_erf\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.5/convffn/act/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Erf_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.5/convffn/act/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.5/convffn/fc1/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.5/convffn/act/Mul_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Constant_2_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Mul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.5/convffn/fc2/Conv",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/transformer.5/convffn/act/Mul_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        2048,
                        1,
                        "unk__52"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "W",
              "value": [
                {
                  "name": "transformer.5.convffn.fc2.weight",
                  "initializer": {
                    "name": "transformer.5.convffn.fc2.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          2048,
                          1,
                          1
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        2048,
                        1,
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "transformer.5.convffn.fc2.bias",
                  "initializer": {
                    "name": "transformer.5.convffn.fc2.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/transformer.5/convffn/fc2/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__53"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "dilations",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            },
            {
              "name": "group",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "1"
              }
            },
            {
              "name": "kernel_shape",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            },
            {
              "name": "pads",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "0",
                  "0",
                  "0"
                ]
              }
            },
            {
              "name": "strides",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "1",
                  "1"
                ]
              }
            }
          ],
          "type": {
            "name": "Conv",
            "module": "ai.onnx",
            "version": 11,
            "description": "The convolution operator consumes an input tensor and a filter, and\ncomputes the output.",
            "attributes": [
              {
                "name": "auto_pad",
                "type": "string",
                "required": false,
                "default": "NOTSET",
                "description": "auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER."
              },
              {
                "name": "dilations",
                "type": "int64[]",
                "required": false,
                "description": "dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis."
              },
              {
                "name": "group",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "number of groups input channels and output channels are divided into."
              },
              {
                "name": "kernel_shape",
                "type": "int64[]",
                "required": false,
                "description": "The shape of the convolution kernel. If not present, should be inferred from input W."
              },
              {
                "name": "pads",
                "type": "int64[]",
                "required": false,
                "description": "Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis."
              },
              {
                "name": "strides",
                "type": "int64[]",
                "required": false,
                "description": "Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis."
              }
            ],
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input data tensor from previous layer; has size (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and width. Note that this is for the 2D image. Otherwise the size is (N x C x D1 x D2 ... x Dn). Optionally, if dimension denotation is in effect, the operation expects input data tensor to arrive with the dimension denotation of [DATA_BATCH, DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE ...]."
              },
              {
                "name": "W",
                "type": "T",
                "description": "The weight tensor that will be used in the convolutions; has size (M x C/group x kH x kW), where C is the number of channels, and kH and kW are the height and width of the kernel, and M is the number of feature maps. For more than 2 dimensions, the kernel shape will be (M x C/group x k1 x k2 x ... x kn), where (k1 x k2 x ... kn) is the dimension of the kernel. Optionally, if dimension denotation is in effect, the operation expects the weight tensor to arrive with the dimension denotation of [FILTER_OUT_CHANNEL, FILTER_IN_CHANNEL, FILTER_SPATIAL, FILTER_SPATIAL ...]. Assuming zero based indices for the shape array, X.shape[1] == (W.shape[1] * group) == C and W.shape[0] mod G == 0. Or in other words FILTER_IN_CHANNEL multiplied by the number of groups should be equal to DATA_CHANNEL and the number of feature maps M should be a multiple of the number of groups G."
              },
              {
                "name": "B",
                "type": "T",
                "option": "optional",
                "description": "Optional 1D bias to be added to the convolution, has size of M."
              }
            ],
            "min_input": 2,
            "max_input": 3,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output data tensor that contains the result of the convolution. The output dimensions are functions of the kernel size, stride size, and pad lengths."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "2 - 3",
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "conv",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[1, 1, 1, 1],\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 21.0, 27.0, 33.0, 24.0],  # (1, 1, 5, 5) output tensor\n                [33.0, 54.0, 63.0, 72.0, 51.0],\n                [63.0, 99.0, 108.0, 117.0, 81.0],\n                [93.0, 144.0, 153.0, 162.0, 111.0],\n                [72.0, 111.0, 117.0, 123.0, 84.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_basic_conv_with_padding\",\n)\n\n# Convolution without padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n    pads=[0, 0, 0, 0],\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 63.0, 72.0],  # (1, 1, 3, 3) output tensor\n                [99.0, 108.0, 117.0],\n                [144.0, 153.0, 162.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_basic_conv_without_padding\",\n)"
              },
              {
                "summary": "conv_with_autopad_same",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 5, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with auto_pad='SAME_LOWER' and strides=2\nnode = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    auto_pad=\"SAME_LOWER\",\n    kernel_shape=[3, 3],\n    strides=[2, 2],\n)\ny = np.array(\n    [[[[12.0, 27.0, 24.0], [63.0, 108.0, 81.0], [72.0, 117.0, 84.0]]]]\n).astype(np.float32)\nexpect(node, inputs=[x, W], outputs=[y], name=\"test_conv_with_autopad_same\")"
              },
              {
                "summary": "conv_with_strides",
                "code": "x = np.array(\n    [\n        [\n            [\n                [0.0, 1.0, 2.0, 3.0, 4.0],  # (1, 1, 7, 5) input tensor\n                [5.0, 6.0, 7.0, 8.0, 9.0],\n                [10.0, 11.0, 12.0, 13.0, 14.0],\n                [15.0, 16.0, 17.0, 18.0, 19.0],\n                [20.0, 21.0, 22.0, 23.0, 24.0],\n                [25.0, 26.0, 27.0, 28.0, 29.0],\n                [30.0, 31.0, 32.0, 33.0, 34.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nW = np.array(\n    [\n        [\n            [\n                [1.0, 1.0, 1.0],  # (1, 1, 3, 3) tensor for convolution weights\n                [1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\n\n# Convolution with strides=2 and padding\nnode_with_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 1, 1, 1],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_padding = np.array(\n    [\n        [\n            [\n                [12.0, 27.0, 24.0],  # (1, 1, 4, 3) output tensor\n                [63.0, 108.0, 81.0],\n                [123.0, 198.0, 141.0],\n                [112.0, 177.0, 124.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_padding,\n    inputs=[x, W],\n    outputs=[y_with_padding],\n    name=\"test_conv_with_strides_padding\",\n)\n\n# Convolution with strides=2 and no padding\nnode_without_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[0, 0, 0, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_without_padding = np.array(\n    [\n        [\n            [\n                [54.0, 72.0],  # (1, 1, 3, 2) output tensor\n                [144.0, 162.0],\n                [234.0, 252.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_without_padding,\n    inputs=[x, W],\n    outputs=[y_without_padding],\n    name=\"test_conv_with_strides_no_padding\",\n)\n\n# Convolution with strides=2 and padding only along one dimension (the H dimension in NxCxHxW tensor)\nnode_with_asymmetric_padding = onnx.helper.make_node(\n    \"Conv\",\n    inputs=[\"x\", \"W\"],\n    outputs=[\"y\"],\n    kernel_shape=[3, 3],\n    pads=[1, 0, 1, 0],\n    strides=[\n        2,\n        2,\n    ],  # Default values for other attributes: dilations=[1, 1], groups=1\n)\ny_with_asymmetric_padding = np.array(\n    [\n        [\n            [\n                [21.0, 33.0],  # (1, 1, 4, 2) output tensor\n                [99.0, 117.0],\n                [189.0, 207.0],\n                [171.0, 183.0],\n            ]\n        ]\n    ]\n).astype(np.float32)\nexpect(\n    node_with_asymmetric_padding,\n    inputs=[x, W],\n    outputs=[y_with_asymmetric_padding],\n    name=\"test_conv_with_strides_and_asymmetric_padding\",\n)"
              }
            ],
            "category": "Layer"
          }
        },
        {
          "name": "/transformer.5/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "transformer.5.layer_scale",
                  "initializer": {
                    "name": "transformer.5.layer_scale",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          1,
                          1
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        1,
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.5/convffn/fc2/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__53"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.5/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__53"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.5/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/transformer.5/token_mixer/reparam_conv/Conv_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__50"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/transformer.5/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__53"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/transformer.5/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__54"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/transformer.5/Squeeze",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.5/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        1,
                        "unk__54"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "axes",
              "value": [
                {
                  "name": "/transformer.5/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 2,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "squeezed",
              "value": [
                {
                  "name": "/transformer.5/Squeeze_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        "unk__54"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Squeeze",
            "module": "ai.onnx",
            "version": 13,
            "description": "Remove single-dimensional entries from the shape of a tensor.\nTakes an input `axes` with a list of axes to squeeze.\nIf `axes` is not provided, all the single dimensions will be removed from\nthe shape. If an axis is selected with shape entry not equal to one, an error is raised.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensors with at least max(dims) dimensions."
              },
              {
                "name": "axes",
                "type": "tensor(int64)",
                "option": "optional",
                "description": "List of integers indicating the dimensions to squeeze. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "min_input": 1,
            "max_input": 2,
            "outputs": [
              {
                "name": "squeezed",
                "type": "T",
                "description": "Reshaped tensor with same data as input."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - 2",
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "squeeze",
                "code": "node = onnx.helper.make_node(\n    \"Squeeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 4, 5).astype(np.float32)\naxes = np.array([0], dtype=np.int64)\ny = np.squeeze(x, axis=0)\n\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_squeeze\")"
              },
              {
                "summary": "squeeze_negative_axes",
                "code": "node = onnx.helper.make_node(\n    \"Squeeze\",\n    inputs=[\"x\", \"axes\"],\n    outputs=[\"y\"],\n)\nx = np.random.randn(1, 3, 1, 5).astype(np.float32)\naxes = np.array([-2], dtype=np.int64)\ny = np.squeeze(x, axis=-2)\nexpect(node, inputs=[x, axes], outputs=[y], name=\"test_squeeze_negative_axes\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/transformer.5/Transpose_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/transformer.5/Squeeze_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        512,
                        "unk__54"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "transposed",
              "value": [
                {
                  "name": "/transformer.5/Transpose_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "perm",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "0",
                  "2",
                  "1"
                ]
              }
            }
          ],
          "type": {
            "name": "Transpose",
            "module": "ai.onnx",
            "version": 13,
            "description": "Transpose the input tensor similar to numpy.transpose. For example, when\nperm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\nwill be (2, 1, 3).\n",
            "attributes": [
              {
                "name": "perm",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "transposed",
                "type": "T",
                "description": "Transposed output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "all_permutations",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\npermutations = list(itertools.permutations(np.arange(len(shape))))\n\nfor i, permutation in enumerate(permutations):\n    node = onnx.helper.make_node(\n        \"Transpose\",\n        inputs=[\"data\"],\n        outputs=[\"transposed\"],\n        perm=permutation,\n    )\n    transposed = np.transpose(data, permutation)\n    expect(\n        node,\n        inputs=[data],\n        outputs=[transposed],\n        name=f\"test_transpose_all_permutations_{i}\",\n    )"
              },
              {
                "summary": "default",
                "code": "shape = (2, 3, 4)\ndata = np.random.random_sample(shape).astype(np.float32)\n\nnode = onnx.helper.make_node(\n    \"Transpose\", inputs=[\"data\"], outputs=[\"transposed\"]\n)\n\ntransposed = np.transpose(data)\nexpect(node, inputs=[data], outputs=[transposed], name=\"test_transpose_default\")"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/final_layer_norm/Cast",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/transformer.5/Transpose_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/final_layer_norm/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/final_layer_norm/ReduceMean",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/final_layer_norm/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/final_layer_norm/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/final_layer_norm/Sub",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/final_layer_norm/Cast_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/final_layer_norm/ReduceMean_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/final_layer_norm/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sub",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary subtraction (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sub",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([3, 2, 1]).astype(np.float32)\nz = x - y  # expected output [-2., 0., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.int16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_int16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint8)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint8\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint16)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint16\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint32\")\n\nx = np.random.randint(12, 24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(12, size=(3, 4, 5), dtype=np.uint64)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_uint64\")"
              },
              {
                "summary": "sub_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Sub\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x - y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_sub_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/final_layer_norm/Pow",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/final_layer_norm/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "Y",
              "value": [
                {
                  "name": "/final_layer_norm/Constant_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Z",
              "value": [
                {
                  "name": "/final_layer_norm/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Pow",
            "module": "ai.onnx",
            "version": 13,
            "description": "Pow takes input data (Tensor<T>) and exponent Tensor, and\nproduces one output data (Tensor<T>) where the function `f(x) = x^exponent`,\nis applied to the data tensor elementwise.\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "First operand, base of the exponent."
              },
              {
                "name": "Y",
                "type": "T1",
                "description": "Second operand, power of the exponent."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Z",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input X and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain input Y types to float/int tensors.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "pow",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_example\")\n\nx = np.arange(60).reshape(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow\")"
              },
              {
                "summary": "pow_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array(2).astype(np.float32)\nz = pow(x, y)  # expected output [1., 4., 9.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_scalar\")\n\nnode = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\nx = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\ny = np.array([1, 2, 3]).astype(np.float32)\n# expected output [[1, 4, 27], [4, 25, 216]]\nz = pow(x, y)\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_bcast_array\")"
              },
              {
                "summary": "types",
                "code": "node = onnx.helper.make_node(\n    \"Pow\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_int32\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_float32\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint64)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint64\")\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.uint32)\nz = pow(x, y)  # expected output [1., 32., 729.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_float32_uint32\")\n\nx = np.array([1, 2, 3]).astype(np.int64)\ny = np.array([4, 5, 6]).astype(np.int64)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int64_int64\")\n\nx = np.array([1, 2, 3]).astype(np.int32)\ny = np.array([4, 5, 6]).astype(np.int32)\nz = pow(x, y)  # expected output [1, 32, 729]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_pow_types_int32_int32\")"
              }
            ]
          }
        },
        {
          "name": "/final_layer_norm/ReduceMean_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/final_layer_norm/Pow_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/final_layer_norm/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axes",
              "type": "int64[]",
              "value": {
                "type": "bigint[]",
                "value": [
                  "-1"
                ]
              }
            }
          ],
          "type": {
            "name": "ReduceMean",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\ntensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\nthe resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\nvalid. Reduction over an empty set of values yields undefined.\n\n\nThe above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\nto `False` instead of `True`.",
            "attributes": [
              {
                "name": "axes",
                "type": "int64[]",
                "required": false,
                "description": "A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "T",
                "description": "Reduced output tensor."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n# print(reduced)\n# [[[18.25]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=None, keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_default_axes_keepdims_random\",\n)"
              },
              {
                "summary": "do_not_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 0\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[12.5, 1.5]\n# [35., 1.5]\n# [57.5, 1.5]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_do_not_keepdims_random\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([1], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axes_keepdims",
                "code": "shape = [3, 2, 2]\naxes = np.array([-2], dtype=np.int64)\nkeepdims = 1\n\nnode = onnx.helper.make_node(\n    \"ReduceMean\",\n    inputs=[\"data\", \"axes\"],\n    outputs=[\"reduced\"],\n    keepdims=keepdims,\n)\n\ndata = np.array(\n    [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n    dtype=np.float32,\n)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n# print(reduced)\n# [[[12.5, 1.5]]\n# [[35., 1.5]]\n# [[57.5, 1.5]]]\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_example\",\n)\n\nnp.random.seed(0)\ndata = np.random.uniform(-10, 10, shape).astype(np.float32)\nreduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\nexpect(\n    node,\n    inputs=[data, axes],\n    outputs=[reduced],\n    name=\"test_reduce_mean_negative_axes_keepdims_random\",\n)"
              }
            ]
          }
        },
        {
          "name": "/final_layer_norm/Add",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/final_layer_norm/ReduceMean_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        1
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/final_layer_norm/Constant_1_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/final_layer_norm/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/final_layer_norm/Sqrt",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "X",
              "value": [
                {
                  "name": "/final_layer_norm/Add_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "/final_layer_norm/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Sqrt",
            "module": "ai.onnx",
            "version": 13,
            "description": "Square root takes one input data (Tensor<T>) and produces one output data\n(Tensor<T>) where the square root is, y = x^0.5, is applied to\nthe tensor elementwise. If x is negative, then it will return NaN.\n",
            "inputs": [
              {
                "name": "X",
                "type": "T",
                "description": "Input tensor"
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Output tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "sqrt",
                "code": "node = onnx.helper.make_node(\n    \"Sqrt\",\n    inputs=[\"x\"],\n    outputs=[\"y\"],\n)\n\nx = np.array([1, 4, 9]).astype(np.float32)\ny = np.sqrt(x)  # expected output [1., 2., 3.]\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt_example\")\n\nx = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\ny = np.sqrt(x)\nexpect(node, inputs=[x], outputs=[y], name=\"test_sqrt\")"
              }
            ]
          }
        },
        {
          "name": "/final_layer_norm/Div",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/final_layer_norm/Sub_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/final_layer_norm/Sqrt_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/final_layer_norm/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Div",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "div",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([3, 4]).astype(np.float32)\ny = np.array([1, 2]).astype(np.float32)\nz = x / y  # expected output [3., 2.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64) + 1\nz = x // y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_uint64\")"
              },
              {
                "summary": "div_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Div\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.rand(5).astype(np.float32) + 1.0\nz = x / y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_div_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/final_layer_norm/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/final_layer_norm/Div_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "final_layer_norm.weight",
                  "initializer": {
                    "name": "final_layer_norm.weight",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/final_layer_norm/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/final_layer_norm/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/final_layer_norm/Mul_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "final_layer_norm.bias",
                  "initializer": {
                    "name": "final_layer_norm.bias",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/final_layer_norm/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/final_layer_norm/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/final_layer_norm/Add_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/final_layer_norm/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float16"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/Shape_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "input_ids",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        "batch_size",
                        "sequence_length"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "shape",
              "value": [
                {
                  "name": "/Shape_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        2
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Shape",
            "module": "ai.onnx",
            "version": 13,
            "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "shape",
                "type": "T1",
                "description": "Shape of the input tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Input tensor can be of arbitrary type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain output to int64 tensor.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "shape",
                "code": "x = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n).astype(np.float32)\ntest_shape(\"_example\", x)  # preserve names of original test cases\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\n\ntest_shape(\"\", x)  # preserve names of original test cases\n\ntest_shape(\"_start_1\", x, start=1)\n\ntest_shape(\"_end_1\", x, end=1)\n\ntest_shape(\"_start_negative_1\", x, start=-1)\n\ntest_shape(\"_end_negative_1\", x, end=-1)\n\ntest_shape(\"_start_1_end_negative_1\", x, start=1, end=-1)\n\ntest_shape(\"_start_1_end_2\", x, start=1, end=2)\n\ntest_shape(\"_clip_start\", x, start=-10)\n\ntest_shape(\"_clip_end\", x, end=10)"
              }
            ]
          }
        },
        {
          "name": "/Gather_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/Shape_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        2
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/Constant_2_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/Cast_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/Gather_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/Cast_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "int64"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/Range",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "start",
              "value": [
                {
                  "name": "/Constant_3_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "limit",
              "value": [
                {
                  "name": "/Cast_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            },
            {
              "name": "delta",
              "value": [
                {
                  "name": "/Constant_4_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": []
                      }
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": []
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/Range_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        "unk__55"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Range",
            "module": "ai.onnx",
            "version": 11,
            "description": "Generate a tensor containing a sequence of numbers that begin at `start` and extends by increments of `delta`\nup to `limit` (exclusive).\n\nThe number of elements in the output of range is computed as below:\n\n```\nnumber_of_elements = max( ceil( (limit - start) / delta ) , 0 )\n```\n\nThe pseudocode determining the contents of the output is shown below:\n\n```\nfor(int i=0; i<number_of_elements; ++i) {\n  output[i] =  start + (i * delta);\n}\n```\n\nExample 1\n\n```\nInputs: start = 3, limit = 9, delta = 3\nOutput: [3, 6]\n```\n\nExample 2\n\n```\nInputs: start = 10, limit = 4, delta = -2\nOutput: [10, 8, 6]\n```\n",
            "inputs": [
              {
                "name": "start",
                "type": "T",
                "description": "Scalar. First entry for the range of output values."
              },
              {
                "name": "limit",
                "type": "T",
                "description": "Scalar. Exclusive upper limit for the range of output values."
              },
              {
                "name": "delta",
                "type": "T",
                "description": "Scalar. Value to step by."
              }
            ],
            "min_input": 3,
            "max_input": 3,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "A 1-D tensor with same type as the inputs containing generated range of values."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types to common numeric type tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "range_float_type_positive_delta",
                "code": "node = onnx.helper.make_node(\n    \"Range\",\n    inputs=[\"start\", \"limit\", \"delta\"],\n    outputs=[\"output\"],\n)\n\nstart = np.float32(1)\nlimit = np.float32(5)\ndelta = np.float32(2)\n\noutput = np.arange(\n    start, limit, delta, dtype=np.float32\n)  # expected output [1.0, 3.0]\nexpect(\n    node,\n    inputs=[start, limit, delta],\n    outputs=[output],\n    name=\"test_range_float_type_positive_delta\",\n)"
              },
              {
                "summary": "range_int32_type_negative_delta",
                "code": "node = onnx.helper.make_node(\n    \"Range\",\n    inputs=[\"start\", \"limit\", \"delta\"],\n    outputs=[\"output\"],\n)\n\nstart = np.int32(10)\nlimit = np.int32(6)\ndelta = np.int32(-3)\n\noutput = np.arange(\n    start, limit, delta, dtype=np.int32\n)  # expected output [10, 7]\nexpect(\n    node,\n    inputs=[start, limit, delta],\n    outputs=[output],\n    name=\"test_range_int32_type_negative_delta\",\n)"
              }
            ]
          }
        },
        {
          "name": "/Cast_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "input_ids",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        "batch_size",
                        "sequence_length"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/Cast_2_output_0",
                  "type": {
                    "dataType": "int32",
                    "shape": {
                      "dimensions": [
                        "batch_size",
                        "sequence_length"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "int32"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        },
        {
          "name": "/ArgMax",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/Cast_2_output_0",
                  "type": {
                    "dataType": "int32",
                    "shape": {
                      "dimensions": [
                        "batch_size",
                        "sequence_length"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reduced",
              "value": [
                {
                  "name": "/ArgMax_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        "batch_size"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "-1"
              }
            },
            {
              "name": "keepdims",
              "type": "int64"
            },
            {
              "name": "select_last_index",
              "type": "int64"
            }
          ],
          "type": {
            "name": "ArgMax",
            "module": "ai.onnx",
            "version": 13,
            "description": "Computes the indices of the max elements of the input tensor's element along the\nprovided axis. The resulting tensor has the same rank as the input if keepdims equals 1.\nIf keepdims equals 0, then the resulting tensor has the reduced dimension pruned.\nIf select_last_index is True (default False), the index of the last occurrence of the max\nis selected if the max appears more than once in the input. Otherwise the index of the\nfirst occurrence is selected.\nThe type of the output tensor is integer.",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "The axis in which to compute the arg indices. Accepted range is [-r, r-1] where r = rank(data)."
              },
              {
                "name": "keepdims",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Keep the reduced dimension or not, default 1 means keep reduced dimension."
              },
              {
                "name": "select_last_index",
                "type": "int64",
                "required": false,
                "description": "Whether to select the last index or the first index if the {name} appears in multiple indices, default is False (first index)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "reduced",
                "type": "tensor(int64)",
                "description": "Reduced output tensor with integer data type."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "default_axes_keepdims",
                "code": "data = np.array([[2, 1], [3, 10]], dtype=np.float32)\nkeepdims = 1\nnode = onnx.helper.make_node(\n    \"ArgMax\", inputs=[\"data\"], outputs=[\"result\"], keepdims=keepdims\n)\n\n# result: [[1, 1]]\nresult = argmax_use_numpy(data, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_default_axis_example\",\n)\n\ndata = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n# result's shape: [1, 3, 4]\nresult = argmax_use_numpy(data, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_default_axis_random\",\n)"
              },
              {
                "summary": "default_axes_keepdims_select_last_index",
                "code": "data = np.array([[2, 2], [3, 10]], dtype=np.float32)\nkeepdims = 1\nnode = onnx.helper.make_node(\n    \"ArgMax\",\n    inputs=[\"data\"],\n    outputs=[\"result\"],\n    keepdims=keepdims,\n    select_last_index=True,\n)\n\n# result: [[1, 1]]\nresult = argmax_use_numpy_select_last_index(data, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_default_axis_example_select_last_index\",\n)\n\ndata = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n# result's shape: [1, 3, 4]\nresult = argmax_use_numpy_select_last_index(data, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_default_axis_random_select_last_index\",\n)"
              },
              {
                "summary": "keepdims",
                "code": "data = np.array([[2, 1], [3, 10]], dtype=np.float32)\naxis = 1\nkeepdims = 1\nnode = onnx.helper.make_node(\n    \"ArgMax\", inputs=[\"data\"], outputs=[\"result\"], axis=axis, keepdims=keepdims\n)\n# result: [[0], [1]]\nresult = argmax_use_numpy(data, axis=axis, keepdims=keepdims)\nexpect(\n    node, inputs=[data], outputs=[result], name=\"test_argmax_keepdims_example\"\n)\n\ndata = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n# result's shape: [2, 1, 4]\nresult = argmax_use_numpy(data, axis=axis, keepdims=keepdims)\nexpect(\n    node, inputs=[data], outputs=[result], name=\"test_argmax_keepdims_random\"\n)"
              },
              {
                "summary": "keepdims_select_last_index",
                "code": "data = np.array([[2, 2], [3, 10]], dtype=np.float32)\naxis = 1\nkeepdims = 1\nnode = onnx.helper.make_node(\n    \"ArgMax\",\n    inputs=[\"data\"],\n    outputs=[\"result\"],\n    axis=axis,\n    keepdims=keepdims,\n    select_last_index=True,\n)\n# result: [[1], [1]]\nresult = argmax_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_keepdims_example_select_last_index\",\n)\n\ndata = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n# result's shape: [2, 1, 4]\nresult = argmax_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_keepdims_random_select_last_index\",\n)"
              },
              {
                "summary": "negative_axis_keepdims",
                "code": "data = np.array([[2, 1], [3, 10]], dtype=np.float32)\naxis = -1\nkeepdims = 1\nnode = onnx.helper.make_node(\n    \"ArgMax\", inputs=[\"data\"], outputs=[\"result\"], axis=axis, keepdims=keepdims\n)\n# result: [[0], [1]]\nresult = argmax_use_numpy(data, axis=axis, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_negative_axis_keepdims_example\",\n)\n\ndata = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n# result's shape: [2, 3, 1]\nresult = argmax_use_numpy(data, axis=axis, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_negative_axis_keepdims_random\",\n)"
              },
              {
                "summary": "negative_axis_keepdims_select_last_index",
                "code": "data = np.array([[2, 2], [3, 10]], dtype=np.float32)\naxis = -1\nkeepdims = 1\nnode = onnx.helper.make_node(\n    \"ArgMax\",\n    inputs=[\"data\"],\n    outputs=[\"result\"],\n    axis=axis,\n    keepdims=keepdims,\n    select_last_index=True,\n)\n# result: [[1], [1]]\nresult = argmax_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_negative_axis_keepdims_example_select_last_index\",\n)\n\ndata = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n# result's shape: [2, 3, 1]\nresult = argmax_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_negative_axis_keepdims_random_select_last_index\",\n)"
              },
              {
                "summary": "no_keepdims",
                "code": "data = np.array([[2, 1], [3, 10]], dtype=np.float32)\naxis = 1\nkeepdims = 0\nnode = onnx.helper.make_node(\n    \"ArgMax\", inputs=[\"data\"], outputs=[\"result\"], axis=axis, keepdims=keepdims\n)\n# result: [0, 1]\nresult = argmax_use_numpy(data, axis=axis, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_no_keepdims_example\",\n)\n\ndata = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n# result's shape: [2, 4]\nresult = argmax_use_numpy(data, axis=axis, keepdims=keepdims)\nexpect(\n    node, inputs=[data], outputs=[result], name=\"test_argmax_no_keepdims_random\"\n)"
              },
              {
                "summary": "no_keepdims_select_last_index",
                "code": "data = np.array([[2, 2], [3, 10]], dtype=np.float32)\naxis = 1\nkeepdims = 0\nnode = onnx.helper.make_node(\n    \"ArgMax\",\n    inputs=[\"data\"],\n    outputs=[\"result\"],\n    axis=axis,\n    keepdims=keepdims,\n    select_last_index=True,\n)\n# result: [1, 1]\nresult = argmax_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_no_keepdims_example_select_last_index\",\n)\n\ndata = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n# result's shape: [2, 4]\nresult = argmax_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\nexpect(\n    node,\n    inputs=[data],\n    outputs=[result],\n    name=\"test_argmax_no_keepdims_random_select_last_index\",\n)"
              }
            ]
          }
        },
        {
          "name": "/Shape_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/final_layer_norm/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "shape",
              "value": [
                {
                  "name": "/Shape_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Shape",
            "module": "ai.onnx",
            "version": 13,
            "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "shape",
                "type": "T1",
                "description": "Shape of the input tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Input tensor can be of arbitrary type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain output to int64 tensor.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "shape",
                "code": "x = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n).astype(np.float32)\ntest_shape(\"_example\", x)  # preserve names of original test cases\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\n\ntest_shape(\"\", x)  # preserve names of original test cases\n\ntest_shape(\"_start_1\", x, start=1)\n\ntest_shape(\"_end_1\", x, end=1)\n\ntest_shape(\"_start_negative_1\", x, start=-1)\n\ntest_shape(\"_end_negative_1\", x, end=-1)\n\ntest_shape(\"_start_1_end_negative_1\", x, start=1, end=-1)\n\ntest_shape(\"_start_1_end_2\", x, start=1, end=2)\n\ntest_shape(\"_clip_start\", x, start=-10)\n\ntest_shape(\"_clip_end\", x, end=10)"
              }
            ]
          }
        },
        {
          "name": "/Gather_2",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/Shape_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/Constant_5_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 1,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/Gather_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/Gather_3",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/Shape_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        3
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/Constant_6_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 2,
                      "1": 0,
                      "2": 0,
                      "3": 0,
                      "4": 0,
                      "5": 0,
                      "6": 0,
                      "7": 0
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/Gather_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/Flatten",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "/final_layer_norm/Cast_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__48",
                        "unk__54",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/Flatten_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__56",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64",
              "value": {
                "type": "bigint",
                "value": "2"
              }
            }
          ],
          "type": {
            "name": "Flatten",
            "module": "ai.onnx",
            "version": 13,
            "description": "Flattens the input tensor into a 2D matrix. If input tensor has shape\n(d_0, d_1, ... d_n) then the output will have shape\n(d_0 X d_1 ... d_(axis-1), d_axis X d_(axis+1) ... X dn).\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "default": 1,
                "description": "Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output. The value for axis must be in the range [-r, r], where r is the rank of the input tensor. Negative value means counting dimensions from the back. When axis = 0, the shape of the output tensor is (1, (d_0 X d_1 ... d_n), where the shape of the input tensor is (d_0, d_1, ... d_n). "
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T",
                "description": "A tensor of rank >= axis."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "A 2D tensor with the contents of the input tensor, with input dimensions up to axis flattened to the outer dimension of the output and remaining input dimensions flattened into the inner dimension of the output."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "flatten",
                "code": "shape = (2, 3, 4, 5)\na = np.random.random_sample(shape).astype(np.float32)\n\nfor i in range(len(shape)):\n    node = onnx.helper.make_node(\n        \"Flatten\",\n        inputs=[\"a\"],\n        outputs=[\"b\"],\n        axis=i,\n    )\n\n    new_shape = (1, -1) if i == 0 else (np.prod(shape[0:i]).astype(int), -1)\n    b = np.reshape(a, new_shape)\n    expect(node, inputs=[a], outputs=[b], name=\"test_flatten_axis\" + str(i))"
              },
              {
                "summary": "flatten_negative_axis",
                "code": "shape = (2, 3, 4, 5)\na = np.random.random_sample(shape).astype(np.float32)\n\nfor i in range(-len(shape), 0):\n    node = onnx.helper.make_node(\n        \"Flatten\",\n        inputs=[\"a\"],\n        outputs=[\"b\"],\n        axis=i,\n    )\n\n    new_shape = (np.prod(shape[0:i]).astype(int), -1)\n    b = np.reshape(a, new_shape)\n    expect(\n        node,\n        inputs=[a],\n        outputs=[b],\n        name=\"test_flatten_negative_axis\" + str(abs(i)),\n    )"
              },
              {
                "summary": "flatten_with_default_axis",
                "code": "node = onnx.helper.make_node(\n    \"Flatten\",\n    inputs=[\"a\"],\n    outputs=[\"b\"],  # Default value for axis: axis=1\n)\n\nshape = (5, 4, 3, 2)\na = np.random.random_sample(shape).astype(np.float32)\nnew_shape = (5, 24)\nb = np.reshape(a, new_shape)\nexpect(node, inputs=[a], outputs=[b], name=\"test_flatten_default_axis\")"
              }
            ],
            "category": "Shape"
          }
        },
        {
          "name": "/Mul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/Range_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        "unk__55"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/Gather_2_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/Mul_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        "unk__55"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Mul",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "mul",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.array([1, 2, 3]).astype(np.float32)\ny = np.array([4, 5, 6]).astype(np.float32)\nz = x * y  # expected output [4., 10., 18.]\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_example\")\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_int16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint8\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint16\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint32\")\n\nx = np.random.randint(4, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_uint64\")"
              },
              {
                "summary": "mul_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Mul\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nz = x * y\nexpect(node, inputs=[x, y], outputs=[z], name=\"test_mul_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/Add_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/ArgMax_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        "batch_size"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "/Mul_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        "unk__55"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "C",
              "value": [
                {
                  "name": "/Add_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        "unk__57"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Add",
            "module": "ai.onnx",
            "version": 14,
            "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n\nThis operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md).\n\n(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "First operand."
              },
              {
                "name": "B",
                "type": "T",
                "description": "Second operand."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "C",
                "type": "T",
                "description": "Result, has same element type as two inputs"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all numeric tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "add",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(3, 4, 5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.int16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_int16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint8)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint8\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint16)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint16\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint32\")\n\nx = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\ny = np.random.randint(24, size=(3, 4, 5), dtype=np.uint64)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_uint64\")"
              },
              {
                "summary": "add_broadcast",
                "code": "node = onnx.helper.make_node(\n    \"Add\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"sum\"],\n)\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = np.random.randn(5).astype(np.float32)\nexpect(node, inputs=[x, y], outputs=[x + y], name=\"test_add_bcast\")"
              }
            ]
          }
        },
        {
          "name": "/Gather_4",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/Flatten_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__56",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "indices",
              "value": [
                {
                  "name": "/Add_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        "unk__57"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "/Gather_4_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__57",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Gather",
            "module": "ai.onnx",
            "version": 13,
            "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\nentries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\nthem in an output tensor of rank q + (r - 1).\n\nIt is an indexing operation that indexes into the input `data` along a single (specified) axis.\nEach entry in `indices` produces a `r-1` dimensional slice of the input tensor.\nThe entire operation produces, conceptually, a `q`-dimensional tensor of `r-1` dimensional slices,\nwhich is arranged into a `q + (r-1)`-dimensional tensor, with the `q` dimensions taking the\nplace of the original `axis` that is being indexed into.\n\nThe following few examples illustrate how `Gather` works for specific shapes of `data`,\n`indices`, and given value of `axis`:\n| data shape | indices shape | axis | output shape | output equation |\n| --- | --- | --- | --- | --- |\n| (P, Q) | ( )  (a scalar)   | 0 | (Q)       | output[q] = data[indices, q] |\n| (P, Q, R) | ( )  (a scalar)   | 1 | (P, R)       | output[p, r] = data[p, indices, r] |\n| (P, Q) | (R, S) | 0 | (R, S, Q) | output[r, s, q] = data[ [indices[r, s], q] |\n| (P, Q) | (R, S) | 1 | (P, R, S) | output[p, r, s] = data[ p, indices[r, s]] |\n\nMore generally, if `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2],\n    [2.3, 3.4],\n    [4.5, 5.7],\n]\nindices = [\n    [0, 1],\n    [1, 2],\n]\noutput = [\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n    ],\n    [\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ],\n]\n```\n\nIf `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1}]`\nthen `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ..., j_{r-2}]`:\n\n```\ndata = [\n    [1.0, 1.2, 1.9],\n    [2.3, 3.4, 3.9],\n    [4.5, 5.7, 5.9],\n]\nindices = [\n    [0, 2],\n]\naxis = 1,\noutput = [\n        [[1.0, 1.9]],\n        [[2.3, 3.9]],\n        [[4.5, 5.9]],\n]\n```\n",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": false,
                "description": "Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data)."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "Tensor of rank r >= 1."
              },
              {
                "name": "indices",
                "type": "Tind",
                "description": "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] along axis of size s. It is an error if any of the index values are out of bounds."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "output",
                "type": "T",
                "description": "Tensor of rank q + (r - 1)."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain indices to integer types",
                "type_param_str": "Tind",
                "allowed_type_strs": [
                  "tensor(int32)",
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "gather_0",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=0)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_0\",\n)"
              },
              {
                "summary": "gather_1",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(5, 4, 3, 2).astype(np.float32)\nindices = np.array([0, 1, 3])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_1\",\n)"
              },
              {
                "summary": "gather_2d_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=1,\n)\ndata = np.random.randn(3, 3).astype(np.float32)\nindices = np.array([[0, 2]])\ny = np.take(data, indices, axis=1)\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_2d_indices\",\n)"
              },
              {
                "summary": "gather_negative_indices",
                "code": "node = onnx.helper.make_node(\n    \"Gather\",\n    inputs=[\"data\", \"indices\"],\n    outputs=[\"y\"],\n    axis=0,\n)\ndata = np.arange(10).astype(np.float32)\nindices = np.array([0, -9, -10])\ny = np.take(data, indices, axis=0)\n\n# print(y)\n# [0. 1. 0.]\n\nexpect(\n    node,\n    inputs=[data, indices.astype(np.int64)],\n    outputs=[y],\n    name=\"test_gather_negative_indices\",\n)"
              }
            ],
            "category": "Transform"
          }
        },
        {
          "name": "/Shape_3",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/Add_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        "unk__57"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "shape",
              "value": [
                {
                  "name": "/Shape_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "Shape",
            "module": "ai.onnx",
            "version": 13,
            "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n",
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "shape",
                "type": "T1",
                "description": "Shape of the input tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Input tensor can be of arbitrary type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              },
              {
                "description": "Constrain output to int64 tensor.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(int64)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "shape",
                "code": "x = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n).astype(np.float32)\ntest_shape(\"_example\", x)  # preserve names of original test cases\n\nx = np.random.randn(3, 4, 5).astype(np.float32)\n\ntest_shape(\"\", x)  # preserve names of original test cases\n\ntest_shape(\"_start_1\", x, start=1)\n\ntest_shape(\"_end_1\", x, end=1)\n\ntest_shape(\"_start_negative_1\", x, start=-1)\n\ntest_shape(\"_end_negative_1\", x, end=-1)\n\ntest_shape(\"_start_1_end_negative_1\", x, start=1, end=-1)\n\ntest_shape(\"_start_1_end_2\", x, start=1, end=2)\n\ntest_shape(\"_clip_start\", x, start=-10)\n\ntest_shape(\"_clip_end\", x, end=10)"
              }
            ]
          }
        },
        {
          "name": "/Concat",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "inputs",
              "value": [
                {
                  "name": "/Constant_7_output_0",
                  "initializer": {
                    "category": "Constant",
                    "encoding": "<",
                    "type": {
                      "dataType": "int64",
                      "shape": {
                        "dimensions": [
                          1
                        ]
                      }
                    },
                    "values": {
                      "0": 255,
                      "1": 255,
                      "2": 255,
                      "3": 255,
                      "4": 255,
                      "5": 255,
                      "6": 255,
                      "7": 255
                    }
                  },
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/Gather_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "concat_result",
              "value": [
                {
                  "name": "/Concat_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        2
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Concat",
            "module": "ai.onnx",
            "version": 13,
            "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": true,
                "description": "Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs).."
              }
            ],
            "inputs": [
              {
                "name": "inputs",
                "type": "T",
                "list": true,
                "description": "List of tensors for concatenation"
              }
            ],
            "min_input": 1,
            "max_input": 2147483647,
            "outputs": [
              {
                "name": "concat_result",
                "type": "T",
                "description": "Concatenated tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - &#8734;",
            "type_constraints": [
              {
                "description": "Constrain output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "concat",
                "code": "test_cases: dict[str, Sequence[Any]] = {\n    \"1d\": ([1, 2], [3, 4]),\n    \"2d\": ([[1, 2], [3, 4]], [[5, 6], [7, 8]]),\n    \"3d\": (\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        [[[9, 10], [11, 12]], [[13, 14], [15, 16]]],\n    ),\n}\n\nfor test_case, values_ in test_cases.items():\n    values = [np.asarray(v, dtype=np.float32) for v in values_]\n    for i in range(len(values[0].shape)):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_\" + str(i),\n        )\n\n    for i in range(-len(values[0].shape), 0):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_negative_\" + str(abs(i)),\n        )"
              }
            ],
            "category": "Tensor"
          }
        },
        {
          "name": "/Reshape",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/Gather_4_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__57",
                        512
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "shape",
              "value": [
                {
                  "name": "/Concat_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        2
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reshaped",
              "value": [
                {
                  "name": "/Reshape_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__58",
                        "unk__59"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "allowzero",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Reshape",
            "module": "ai.onnx",
            "version": 14,
            "description": "Reshape the input tensor similar to numpy.reshape.\nFirst input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\nAt most one dimension of the new shape can be -1. In this case, the value is\ninferred from the size of the tensor and the remaining dimensions. A dimension\ncould also be 0, in which case the actual dimension value is unchanged (i.e. taken\nfrom the input tensor). If 'allowzero' is set, and the new shape includes 0, the\ndimension will be set explicitly to zero (i.e. not taken from input tensor).\nShape (second input) could be an empty shape, which means converting to a scalar.\nThe input tensor's shape and the output tensor's shape are required to have the same number of elements.\n\nIf the attribute 'allowzero' is set, it is invalid for the specified shape to\ncontain both a zero value and -1, as the value of the dimension corresponding\nto -1 cannot be determined uniquely.\n",
            "attributes": [
              {
                "name": "allowzero",
                "type": "int64",
                "required": false,
                "description": "(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              },
              {
                "name": "shape",
                "type": "tensor(int64)",
                "description": "Specified shape for output."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "reshaped",
                "type": "T",
                "description": "Reshaped data."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "allowzero",
                "code": "original_shape = [0, 3, 4]\ntest_cases = {\n    \"allowzero_reordered\": np.array([3, 4, 0], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n        allowzero=1,  # if allowzero=1, final shape = (3, 4, 0)\n        # if allowzero=0, final shape = (3, 4, 4)\n    )\n\n    reshaped = reshape_reference_implementation(data, shape, allowzero=1)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              },
              {
                "summary": "reshape",
                "code": "original_shape = [2, 3, 4]\ntest_cases = {\n    \"reordered_all_dims\": np.array([4, 2, 3], dtype=np.int64),\n    \"reordered_last_dims\": np.array([2, 4, 3], dtype=np.int64),\n    \"reduced_dims\": np.array([2, 12], dtype=np.int64),\n    \"extended_dims\": np.array([2, 3, 2, 2], dtype=np.int64),\n    \"one_dim\": np.array([24], dtype=np.int64),\n    \"negative_dim\": np.array([2, -1, 2], dtype=np.int64),\n    \"negative_extended_dims\": np.array([-1, 2, 3, 4], dtype=np.int64),\n    \"zero_dim\": np.array([2, 0, 4, 1], dtype=np.int64),\n    \"zero_and_negative_dim\": np.array([2, 0, 1, -1], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n    )\n\n    reshaped = reshape_reference_implementation(data, shape)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              }
            ],
            "category": "Shape"
          }
        },
        {
          "name": "/Concat_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "inputs",
              "value": [
                {
                  "name": "/Shape_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                },
                {
                  "name": "/Gather_3_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        1
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "concat_result",
              "value": [
                {
                  "name": "/Concat_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        2
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "axis",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Concat",
            "module": "ai.onnx",
            "version": 13,
            "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
            "attributes": [
              {
                "name": "axis",
                "type": "int64",
                "required": true,
                "description": "Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs).."
              }
            ],
            "inputs": [
              {
                "name": "inputs",
                "type": "T",
                "list": true,
                "description": "List of tensors for concatenation"
              }
            ],
            "min_input": 1,
            "max_input": 2147483647,
            "outputs": [
              {
                "name": "concat_result",
                "type": "T",
                "description": "Concatenated tensor"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "inputs_range": "1 - &#8734;",
            "type_constraints": [
              {
                "description": "Constrain output types to any tensor type.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "concat",
                "code": "test_cases: dict[str, Sequence[Any]] = {\n    \"1d\": ([1, 2], [3, 4]),\n    \"2d\": ([[1, 2], [3, 4]], [[5, 6], [7, 8]]),\n    \"3d\": (\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        [[[9, 10], [11, 12]], [[13, 14], [15, 16]]],\n    ),\n}\n\nfor test_case, values_ in test_cases.items():\n    values = [np.asarray(v, dtype=np.float32) for v in values_]\n    for i in range(len(values[0].shape)):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_\" + str(i),\n        )\n\n    for i in range(-len(values[0].shape), 0):\n        in_args = [\"value\" + str(k) for k in range(len(values))]\n        node = onnx.helper.make_node(\n            \"Concat\", inputs=list(in_args), outputs=[\"output\"], axis=i\n        )\n        output = np.concatenate(values, i)\n        expect(\n            node,\n            inputs=list(values),\n            outputs=[output],\n            name=\"test_concat_\" + test_case + \"_axis_negative_\" + str(abs(i)),\n        )"
              }
            ],
            "category": "Tensor"
          }
        },
        {
          "name": "/Reshape_1",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "data",
              "value": [
                {
                  "name": "/Reshape_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__58",
                        "unk__59"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "shape",
              "value": [
                {
                  "name": "/Concat_1_output_0",
                  "type": {
                    "dataType": "int64",
                    "shape": {
                      "dimensions": [
                        2
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "reshaped",
              "value": [
                {
                  "name": "/Reshape_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__60",
                        "unk__61"
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "allowzero",
              "type": "int64"
            }
          ],
          "type": {
            "name": "Reshape",
            "module": "ai.onnx",
            "version": 14,
            "description": "Reshape the input tensor similar to numpy.reshape.\nFirst input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\nAt most one dimension of the new shape can be -1. In this case, the value is\ninferred from the size of the tensor and the remaining dimensions. A dimension\ncould also be 0, in which case the actual dimension value is unchanged (i.e. taken\nfrom the input tensor). If 'allowzero' is set, and the new shape includes 0, the\ndimension will be set explicitly to zero (i.e. not taken from input tensor).\nShape (second input) could be an empty shape, which means converting to a scalar.\nThe input tensor's shape and the output tensor's shape are required to have the same number of elements.\n\nIf the attribute 'allowzero' is set, it is invalid for the specified shape to\ncontain both a zero value and -1, as the value of the dimension corresponding\nto -1 cannot be determined uniquely.\n",
            "attributes": [
              {
                "name": "allowzero",
                "type": "int64",
                "required": false,
                "description": "(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy."
              }
            ],
            "inputs": [
              {
                "name": "data",
                "type": "T",
                "description": "An input tensor."
              },
              {
                "name": "shape",
                "type": "tensor(int64)",
                "description": "Specified shape for output."
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "reshaped",
                "type": "T",
                "description": "Reshaped data."
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to all tensor types.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)",
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(string)",
                  "tensor(bool)",
                  "tensor(complex64)",
                  "tensor(complex128)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "allowzero",
                "code": "original_shape = [0, 3, 4]\ntest_cases = {\n    \"allowzero_reordered\": np.array([3, 4, 0], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n        allowzero=1,  # if allowzero=1, final shape = (3, 4, 0)\n        # if allowzero=0, final shape = (3, 4, 4)\n    )\n\n    reshaped = reshape_reference_implementation(data, shape, allowzero=1)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              },
              {
                "summary": "reshape",
                "code": "original_shape = [2, 3, 4]\ntest_cases = {\n    \"reordered_all_dims\": np.array([4, 2, 3], dtype=np.int64),\n    \"reordered_last_dims\": np.array([2, 4, 3], dtype=np.int64),\n    \"reduced_dims\": np.array([2, 12], dtype=np.int64),\n    \"extended_dims\": np.array([2, 3, 2, 2], dtype=np.int64),\n    \"one_dim\": np.array([24], dtype=np.int64),\n    \"negative_dim\": np.array([2, -1, 2], dtype=np.int64),\n    \"negative_extended_dims\": np.array([-1, 2, 3, 4], dtype=np.int64),\n    \"zero_dim\": np.array([2, 0, 4, 1], dtype=np.int64),\n    \"zero_and_negative_dim\": np.array([2, 0, 1, -1], dtype=np.int64),\n}\ndata = np.random.random_sample(original_shape).astype(np.float32)\n\nfor test_name, shape in test_cases.items():\n    node = onnx.helper.make_node(\n        \"Reshape\",\n        inputs=[\"data\", \"shape\"],\n        outputs=[\"reshaped\"],\n    )\n\n    reshaped = reshape_reference_implementation(data, shape)\n\n    expect(\n        node,\n        inputs=[data, shape],\n        outputs=[reshaped],\n        name=\"test_reshape_\" + test_name,\n    )"
              }
            ],
            "category": "Shape"
          }
        },
        {
          "name": "/MatMul",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "A",
              "value": [
                {
                  "name": "/Reshape_1_output_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "unk__60",
                        "unk__61"
                      ]
                    }
                  }
                }
              ]
            },
            {
              "name": "B",
              "value": [
                {
                  "name": "projection_layer",
                  "initializer": {
                    "name": "projection_layer",
                    "category": "Initializer",
                    "encoding": "<",
                    "type": {
                      "dataType": "float16",
                      "shape": {
                        "dimensions": [
                          512,
                          512
                        ]
                      }
                    }
                  },
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        512,
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "Y",
              "value": [
                {
                  "name": "graph_output_cast_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "batch_size",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [],
          "type": {
            "name": "MatMul",
            "module": "ai.onnx",
            "version": 13,
            "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
            "inputs": [
              {
                "name": "A",
                "type": "T",
                "description": "N-dimensional matrix A"
              },
              {
                "name": "B",
                "type": "T",
                "description": "N-dimensional matrix B"
              }
            ],
            "min_input": 2,
            "max_input": 2,
            "outputs": [
              {
                "name": "Y",
                "type": "T",
                "description": "Matrix multiply results from A * B"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input and output types to float/int tensors.",
                "type_param_str": "T",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "matmul",
                "code": "node = onnx.helper.make_node(\n    \"MatMul\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"c\"],\n)\n\n# 2d\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_2d\")\n\n# 3d\na = np.random.randn(2, 3, 4).astype(np.float32)\nb = np.random.randn(2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_3d\")\n\n# 4d\na = np.random.randn(1, 2, 3, 4).astype(np.float32)\nb = np.random.randn(1, 2, 4, 3).astype(np.float32)\nc = np.matmul(a, b)\nexpect(node, inputs=[a, b], outputs=[c], name=\"test_matmul_4d\")"
              }
            ]
          }
        },
        {
          "name": "graph_output_cast0",
          "chain": [],
          "metadata": [],
          "inputs": [
            {
              "name": "input",
              "value": [
                {
                  "name": "graph_output_cast_0",
                  "type": {
                    "dataType": "float16",
                    "shape": {
                      "dimensions": [
                        "batch_size",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "outputs": [
            {
              "name": "output",
              "value": [
                {
                  "name": "text_embeds",
                  "type": {
                    "dataType": "float32",
                    "shape": {
                      "dimensions": [
                        "batch_size",
                        512
                      ]
                    }
                  }
                }
              ]
            }
          ],
          "attributes": [
            {
              "name": "to",
              "type": "DataType",
              "value": "float32"
            }
          ],
          "type": {
            "name": "Cast",
            "module": "ai.onnx",
            "version": 13,
            "description": "The operator casts the elements of a given input tensor to a data type\nspecified by the 'to' argument and returns an output tensor of the same size in\nthe converted type. The 'to' argument must be one of the data types specified\nin the 'DataType' enum field in the TensorProto message.\n\nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is supported. For example, converting string \"100.5\" to an integer may\nyield result 100. There are some string literals reserved for special floating-point values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity, negative infinity, and not-a-number, respectively.\nAny string which can exactly match \"+INF\" in a case-insensitive way would be mapped to positive infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal string such as \"Hello World!\" is an undefined behavior. Cases\nof converting string representing floating-point arithmetic value, such as \"2.718\", to INT is an undefined behavior.\n\nConversion from a numerical type to any numerical type is always allowed.\nUser must be aware of precision loss and value change caused by range difference between two types.\nFor example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\nan integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n\nIn more detail, the conversion among numerical types should follow these rules:\n\n* Casting from floating point to:\n  * floating point: +/- infinity if OOR (out of range).\n  * fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to True.\n* Casting from fixed point to:\n  * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n    signed types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero to False; nonzero to True.\n* Casting from bool to:\n  * floating point: `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n",
            "attributes": [
              {
                "name": "to",
                "type": "DataType",
                "required": true,
                "description": "The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto"
              }
            ],
            "inputs": [
              {
                "name": "input",
                "type": "T1",
                "description": "Input tensor to be cast."
              }
            ],
            "min_input": 1,
            "max_input": 1,
            "outputs": [
              {
                "name": "output",
                "type": "T2",
                "description": "Output tensor with the same shape as input with type specified by the 'to' argument"
              }
            ],
            "min_output": 1,
            "max_output": 1,
            "type_constraints": [
              {
                "description": "Constrain input types. Casting from complex is not supported.",
                "type_param_str": "T1",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              },
              {
                "description": "Constrain output types. Casting to complex is not supported.",
                "type_param_str": "T2",
                "allowed_type_strs": [
                  "tensor(float16)",
                  "tensor(float)",
                  "tensor(double)",
                  "tensor(int8)",
                  "tensor(int16)",
                  "tensor(int32)",
                  "tensor(int64)",
                  "tensor(uint8)",
                  "tensor(uint16)",
                  "tensor(uint32)",
                  "tensor(uint64)",
                  "tensor(bool)",
                  "tensor(string)",
                  "tensor(bfloat16)"
                ]
              }
            ],
            "examples": [
              {
                "summary": "cast",
                "code": "shape = (3, 4)\ntest_cases = [\n    (\"FLOAT\", \"FLOAT16\"),\n    (\"FLOAT\", \"DOUBLE\"),\n    (\"FLOAT16\", \"FLOAT\"),\n    (\"FLOAT16\", \"DOUBLE\"),\n    (\"DOUBLE\", \"FLOAT\"),\n    (\"DOUBLE\", \"FLOAT16\"),\n    (\"FLOAT\", \"STRING\"),\n    (\"STRING\", \"FLOAT\"),\n    (\"FLOAT\", \"BFLOAT16\"),\n    (\"BFLOAT16\", \"FLOAT\"),\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT\"),\n    (\"FLOAT8E4M3FN\", \"FLOAT16\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E4M3FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT8E5M2\", \"FLOAT\"),\n    (\"FLOAT8E5M2\", \"FLOAT16\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT\"),\n    (\"FLOAT8E5M2FNUZ\", \"FLOAT16\"),\n    (\"FLOAT\", \"UINT4\"),\n    (\"FLOAT16\", \"UINT4\"),\n    (\"FLOAT\", \"INT4\"),\n    (\"FLOAT16\", \"INT4\"),\n    (\"UINT4\", \"FLOAT\"),\n    (\"UINT4\", \"FLOAT16\"),\n    (\"UINT4\", \"UINT8\"),\n    (\"INT4\", \"FLOAT\"),\n    (\"INT4\", \"FLOAT16\"),\n    (\"INT4\", \"INT8\"),\n    (\"FLOAT4E2M1\", \"FLOAT\"),\n    (\"FLOAT4E2M1\", \"FLOAT16\"),\n    (\"FLOAT\", \"FLOAT4E2M1\"),\n    (\"FLOAT16\", \"FLOAT4E2M1\"),\n]\n\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\nvect_float32_to_uint4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=False)\n)\nvect_float32_to_int4 = np.vectorize(\n    lambda x: subbyte.float32_to_4bit_unpacked(x, signed=True)\n)\n\nf8_types = (\"FLOAT8E4M3FN\", \"FLOAT8E4M3FNUZ\", \"FLOAT8E5M2\", \"FLOAT8E5M2FNUZ\")\n\nfor from_type, to_type in test_cases:\n    input_type_proto = None\n    output_type_proto = None\n    if from_type == \"BFLOAT16\" or to_type == \"BFLOAT16\":\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.float32,\n        )\n        little_endisan = sys.byteorder == \"little\"\n        np_uint16_view = np_fp32.view(dtype=np.uint16)\n        np_bfp16 = (\n            np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n        )\n        if to_type == \"BFLOAT16\":\n            assert from_type == \"FLOAT\"\n            input = np_fp32.reshape([3, 4])\n            output = np_bfp16.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), output.shape\n            )\n        else:\n            assert to_type == \"FLOAT\"\n            input = np_bfp16.reshape([3, 4])\n            # convert bfloat to FLOAT\n            np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n            if little_endisan:\n                np_fp32_zeros[1::2] = np_bfp16\n            else:\n                np_fp32_zeros[0::2] = np_bfp16\n            np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n            output = np_fp32_from_bfloat.reshape([3, 4])\n            input_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.BFLOAT16), input.shape\n            )\n            output_type_proto = onnx.helper.make_tensor_type_proto(\n                int(TensorProto.FLOAT), output.shape\n            )\n    elif from_type in f8_types or to_type in f8_types:\n        np_fp32 = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.7229038\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-0.0000001\",\n                \"0.0000001\",\n                \"-1000000\",\n            ],\n            dtype=np.float32,\n        )\n\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FN\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FN, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E4M3FNUZ\":\n            input_values = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(np_fp32, uz=True), uz=True\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E4M3FNUZ, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32)\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2, [3, 5], input_values.tolist()\n            )\n        elif from_type == \"FLOAT8E5M2FNUZ\":\n            input_values = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(np_fp32, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT8E5M2FNUZ, [3, 5], input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type == \"FLOAT8E4M3FN\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values)\n            )\n        elif to_type == \"FLOAT8E4M3FNUZ\":\n            expected = float8e4m3_to_float32(\n                vect_float32_to_float8e4m3(input_values, uz=True), uz=True\n            )\n        elif to_type == \"FLOAT8E5M2\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values)\n            )\n        elif to_type == \"FLOAT8E5M2FNUZ\":\n            expected = float8e5m2_to_float32(\n                vect_float32_to_float8e5m2(input_values, fn=True, uz=True),\n                fn=True,\n                uz=True,\n            )\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16).astype(np.float32)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"x\", getattr(TensorProto, to_type), [3, 5], expected.tolist()\n        )\n        output = expected_tensor\n    elif from_type in (\"UINT4\", \"INT4\") or to_type in (\"UINT4\", \"INT4\"):\n        np_fp32 = np.arange(-9, 16).astype(np.float32)\n        input_shape = (5, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"UINT4\":\n            input_values = vect_float32_to_uint4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.UINT4, input_shape, input_values.tolist()\n            )\n        elif from_type == \"INT4\":\n            input_values = vect_float32_to_int4(np_fp32)\n            input = make_tensor(\n                \"x\", TensorProto.INT4, input_shape, input_values.tolist()\n            )\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        if to_type == \"UINT4\":\n            expected = vect_float32_to_uint4(input_values).astype(custom.uint4)\n        elif to_type == \"INT4\":\n            expected = vect_float32_to_int4(input_values).astype(custom.int4)\n        elif to_type == \"FLOAT16\":\n            expected = input_values.astype(np.float16)\n        elif to_type == \"FLOAT\":\n            expected = input_values\n        elif to_type == \"UINT8\":\n            expected = input_values.astype(np.uint8)\n        elif to_type == \"INT8\":\n            expected = input_values.astype(np.int8)\n        else:\n            raise ValueError(\n                \"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected_tensor = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n        output = expected_tensor\n        input_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, from_type), input_shape\n        )\n        output_type_proto = onnx.helper.make_tensor_type_proto(\n            getattr(TensorProto, to_type), input_shape\n        )\n    elif from_type == \"FLOAT4E2M1\" or to_type == \"FLOAT4E2M1\":\n        np_fp32 = np.array(\n            [\n                \"0.48\",\n                \"0.25\",\n                \"1.05\",\n                \"-3.5\",\n                \"-8\",\n                \"9\",\n                \"1000000\",\n                \"1e-7\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n                \"-4\",\n                \"0.01\",\n                \"-0.0\",\n            ],\n            dtype=np.float32,\n        )\n        input_shape = (3, 5)\n        if from_type == \"FLOAT\":\n            input_values = np_fp32\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT16\":\n            input_values = np_fp32.astype(np.float16).astype(np.float32)\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT16, input_shape, input_values.tolist()\n            )\n        elif from_type == \"FLOAT4E2M1\":\n            input = make_tensor(\n                \"x\", TensorProto.FLOAT4E2M1, input_shape, np_fp32.tolist()\n            )\n        else:\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n\n        if to_type not in (\"FLOAT\", \"FLOAT16\", \"FLOAT4E2M1\"):\n            raise ValueError(\n                f\"Conversion from {from_type} to {to_type} is not tested.\"\n            )\n        expected = unpacked_float4e2m1_to_float32(\n            subbyte.float32_to_float4e2m1_unpacked(np_fp32)\n        )\n        output = make_tensor(\n            \"y\", getattr(TensorProto, to_type), input_shape, expected.tolist()\n        )\n    elif from_type != \"STRING\":\n        input = np.random.random_sample(shape).astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, from_type))\n        )\n        if to_type == \"STRING\":\n            # Converting input to str, then give it object dtype for generating script\n            ss = []\n            for i in input.flatten():\n                s = str(i).encode(\"utf-8\")\n                su = s.decode(\"utf-8\")\n                ss.append(su)\n\n            output = np.array(ss).astype(object).reshape([3, 4])\n        else:\n            output = input.astype(\n                helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n            )\n    else:\n        input = np.array(\n            [\n                \"0.47892547\",\n                \"0.48033667\",\n                \"0.49968487\",\n                \"0.81910545\",\n                \"0.47031248\",\n                \"0.816468\",\n                \"0.21087195\",\n                \"0.7229038\",\n                \"NaN\",\n                \"INF\",\n                \"+INF\",\n                \"-INF\",\n            ],\n            dtype=np.dtype(object),\n        ).reshape([3, 4])\n        output = input.astype(\n            helper.tensor_dtype_to_np_dtype(getattr(TensorProto, to_type))\n        )\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n    )\n    if input_type_proto and output_type_proto:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n            input_type_protos=[input_type_proto],\n            output_type_protos=[output_type_proto],\n        )\n    else:\n        expect(\n            node,\n            inputs=[input],\n            outputs=[output],\n            name=\"test_cast_\" + from_type + \"_to_\" + to_type,\n        )"
              },
              {
                "summary": "saturate_false",
                "code": "test_cases = [\n    (\"FLOAT\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FN\"),\n    (\"FLOAT\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E4M3FNUZ\"),\n    (\"FLOAT\", \"FLOAT8E5M2\"),\n    (\"FLOAT16\", \"FLOAT8E5M2\"),\n    (\"FLOAT\", \"FLOAT8E5M2FNUZ\"),\n    (\"FLOAT16\", \"FLOAT8E5M2FNUZ\"),\n]\nvect_float32_to_float8e4m3 = np.vectorize(float32_to_float8e4m3)\nvect_float32_to_float8e5m2 = np.vectorize(float32_to_float8e5m2)\n\nfor from_type, to_type in test_cases:\n    np_fp32 = np.array(\n        [\n            \"0.47892547\",\n            \"0.48033667\",\n            \"0.49968487\",\n            \"0.81910545\",\n            \"0.47031248\",\n            \"0.7229038\",\n            \"1000000\",\n            \"1e-7\",\n            \"NaN\",\n            \"INF\",\n            \"+INF\",\n            \"-INF\",\n            \"-0.0000001\",\n            \"0.0000001\",\n            \"-1000000\",\n        ],\n        dtype=np.float32,\n    )\n\n    if from_type == \"FLOAT\":\n        input_values = np_fp32\n        input = make_tensor(\"x\", TensorProto.FLOAT, [3, 5], np_fp32.tolist())\n    elif from_type == \"FLOAT16\":\n        input_values = np_fp32.astype(np.float16).astype(np.float32)\n        input = make_tensor(\n            \"x\", TensorProto.FLOAT16, [3, 5], input_values.tolist()\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    if to_type == \"FLOAT8E4M3FN\":\n        expected = vect_float32_to_float8e4m3(input_values, saturate=False)\n    elif to_type == \"FLOAT8E4M3FNUZ\":\n        expected = vect_float32_to_float8e4m3(\n            input_values, uz=True, saturate=False\n        )\n    elif to_type == \"FLOAT8E5M2\":\n        expected = vect_float32_to_float8e5m2(input_values, saturate=False)\n    elif to_type == \"FLOAT8E5M2FNUZ\":\n        expected = vect_float32_to_float8e5m2(\n            input_values, fn=True, uz=True, saturate=False\n        )\n    else:\n        raise ValueError(\n            \"Conversion from {from_type} to {to_type} is not tested.\"\n        )\n\n    ivals = bytes([int(i) for i in expected])\n    tensor = TensorProto()\n    tensor.data_type = getattr(TensorProto, to_type)\n    tensor.name = \"x\"\n    tensor.dims.extend([3, 5])\n    field = tensor_dtype_to_field(tensor.data_type)\n    getattr(tensor, field).extend(ivals)\n\n    output = tensor\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        inputs=[\"input\"],\n        outputs=[\"output\"],\n        to=getattr(TensorProto, to_type),\n        saturate=0,\n    )\n    expect(\n        node,\n        inputs=[input],\n        outputs=[output],\n        name=\"test_cast_no_saturate_\" + from_type + \"_to_\" + to_type,\n    )"
              }
            ]
          }
        }
      ]
    }
  ]
}